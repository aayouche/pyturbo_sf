{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0227023d",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad134c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import shift\n",
    "import numpy as np\n",
    "from joblib import Parallel,delayed\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import bottleneck as bn\n",
    "from matplotlib.colors import LogNorm, SymLogNorm\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.stats import binned_statistic_2d\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy.stats\n",
    "from dask.distributed import LocalCluster, Client\n",
    "from tools import lagged_difference\n",
    "linewidth = 2\n",
    "fontsize = 12\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.major.width'] = 2\n",
    "plt.rcParams['xtick.minor.width'] = 2\n",
    "plt.rcParams['ytick.major.width'] = 2\n",
    "plt.rcParams['ytick.minor.width'] = 2\n",
    "plt.rcParams['xtick.major.size'] = 10\n",
    "plt.rcParams['xtick.minor.size'] = 5\n",
    "plt.rcParams['ytick.major.size'] = 10\n",
    "plt.rcParams['ytick.minor.size'] = 5\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "plt.rc('font', family='serif')\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a411d9",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ca042",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '/home/aayouche/warrior/data'\n",
    "idt = 10\n",
    "dsh = xr.open_dataset(path_data+'/'+'surface.nc')\n",
    "dsg = xr.open_dataset(path_data+'/'+'GOM_150_grd.nc')\n",
    "\n",
    "x = np.cumsum((1/dsg.pm.values),axis=1)[0,:]\n",
    "y = np.cumsum((1/dsg.pn.values),axis=0)[:,0]\n",
    "\n",
    "x_psi = 0.5*(x[1:] + x[:-1])\n",
    "y_psi = 0.5*(y[1:] + y[:-1])\n",
    "\n",
    "u = np.array(dsh.u.isel(s_rho=0,time=idt))\n",
    "u = 0.5*(u[1:,:] + u[:-1,:])\n",
    "v = np.array(dsh.v.isel(s_rho=0,time=idt))\n",
    "v = 0.5*(v[:,1:] + v[:,:-1])\n",
    "\n",
    "dudx = np.gradient(u,axis=1)/(2*150.)\n",
    "dvdx = np.gradient(v,axis=1)/(2*150.)\n",
    "dudy = np.gradient(u,axis=0)/(2*150.)\n",
    "dvdy = np.gradient(v,axis=0)/(2*150.)\n",
    "\n",
    "adv_u = u*dudx + v*dudy\n",
    "\n",
    "adv_v = v*dudx + v*dvdy\n",
    "\n",
    "temp = np.array(dsh.temp.isel(s_rho=0,time=idt))\n",
    "\n",
    "temp = 0.25*(temp[1:,1:] + temp[1:,:-1] + temp[:-1,:-1] + temp[:-1,1:])\n",
    "\n",
    "X,Y = np.meshgrid(x_psi,y_psi)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27f597-8453-45b3-8c21-541f15363a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1819a1-43b3-4c66-a595-3c8372449577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3057628d",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the xarray dataset\n",
    "ds = xr.Dataset({\n",
    "    'u': (['y', 'x'], u),\n",
    "    'v': (['y', 'x'], v),\n",
    "    'adv_u':(['y', 'x'], adv_u),\n",
    "    'adv_v':(['y', 'x'], adv_v),\n",
    "    'temperature':(['y', 'x'], temp),\n",
    "\n",
    "}, coords={\n",
    "    'x': (['y', 'x'], X),\n",
    "    'y': (['y', 'x'], Y)\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3568a659-4fd3-43e2-972d-693c45aa1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade5cdea",
   "metadata": {},
   "source": [
    "# Plot The Kinetic Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e1121",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(6,6))\n",
    "im = ax.pcolormesh(x_psi/1e3,y_psi/1e3,0.5*(u**2 + v**2),cmap='jet',shading='gouraud')\n",
    "\n",
    "pos1cb = ax.get_position()\n",
    "cbar_ax = fig.add_axes([pos1cb.x0+pos1cb.width/4., pos1cb.y0+0.04+pos1cb.height , pos1cb.width/2. , 0.009])\n",
    "cbar = fig.colorbar(im, cax=cbar_ax,orientation='horizontal',\\\n",
    "                            extend='both',ticks=np.arange(0,1,0.2))\n",
    "cbar.set_label(r'$ {\\rm KE \\,  [} {\\rm m^{2}~} {\\rm s^{-2}]}$',fontsize= fontsize + 2 ,labelpad=10)\n",
    "cbar.ax.tick_params(labelsize=fontsize,direction='in',width=linewidth,size=4)\n",
    "cbar.ax.xaxis.set_label_position('top')\n",
    "cbar.ax.xaxis.set_ticks_position('both')\n",
    "cbar.outline.set_linewidth( linewidth )\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax.spines[axis].set_linewidth( linewidth )\n",
    "    ax.spines[axis].set_color('k')\n",
    "ax.tick_params(direction='in',bottom='on',top='on',left='on',right='on')\n",
    "ax.set_xlabel('X [km]')\n",
    "ax.set_ylabel('Y [km]')\n",
    "fig.savefig('KE.jpg',dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023882f-09e2-49ea-ba90-16743f0d1bdf",
   "metadata": {},
   "source": [
    "# Class Sfun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b4848-500c-4b9b-9996-afdd6bcd43d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import bottleneck as bn\n",
    "from joblib import Parallel, delayed\n",
    "import xarray as xr\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "def fast_shift(input_array, y_shift=0, x_shift=0):  # noqa: D417\n",
    "    \"\"\"\n",
    "    shift 2D array in x and y by the specified integer amounts and returns\n",
    "    the shifted arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        input_array: array_like\n",
    "            2-dimensional array to be shifted.\n",
    "        shift_x: int, optional\n",
    "            Shift amount for x shift.\n",
    "        shift_y: int, optional\n",
    "            Shift amount for y shift.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        shifted_xy_array\n",
    "            2D array shifted in the x-y directions by the specified integer amount\n",
    "    \"\"\"\n",
    "    shifted_xy_array = np.full(np.shape(input_array), np.nan)\n",
    "\n",
    "    if x_shift == 0 and y_shift == 0:\n",
    "        shifted_xy_array = input_array\n",
    "    elif y_shift == 0:\n",
    "        shifted_xy_array[:, :-x_shift] = input_array[:, x_shift:]\n",
    "    elif x_shift == 0:\n",
    "        shifted_xy_array[:-y_shift, :] = input_array[y_shift:, :]\n",
    "    else:\n",
    "        shifted_xy_array[:-y_shift, :-x_shift] = input_array[y_shift:, x_shift:]\n",
    "\n",
    "    return shifted_xy_array\n",
    "    \n",
    "class SFun():\n",
    "    \n",
    "    def __init__(self, ds, bootsize=None):\n",
    "        \"\"\"\n",
    "        Initialize the SFun class with a dataset and optional bootsize\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ds : xarray.Dataset\n",
    "            Dataset containing velocity components and/or scalar fields\n",
    "        bootsize : dict, optional\n",
    "            Dictionary with dimensions as keys and bootsize as values\n",
    "        \"\"\"\n",
    "        self.ds = ds\n",
    "        self.data_dims = list(ds.dims)\n",
    "        \n",
    "        # Check dimensions and transpose if needed\n",
    "        self._check_and_transpose_dimensions()\n",
    "        \n",
    "        # Get data shape from dataset dimensions\n",
    "        self.data_shape = dict(self.ds.dims)\n",
    "        \n",
    "        # Print the shapes for debugging\n",
    "        print(f\"Data dimensions and shapes: {self.data_shape}\")\n",
    "        \n",
    "        # Default bootsize if not provided\n",
    "        if bootsize is None:\n",
    "            self.bootsize = {dim: min(32, self.data_shape[dim] // 2) for dim in self.data_dims}\n",
    "        else:\n",
    "            self.bootsize = bootsize\n",
    "        \n",
    "        print(f\"Using bootsize: {self.bootsize}\")\n",
    "        \n",
    "        # Determine spacings automatically based on bootsizes\n",
    "        self.spacings_info = self._get_simplified_adaptive_spacings()\n",
    "        \n",
    "        # Set default spacing to 1 for initialization\n",
    "        self.spacing = {dim: 1 for dim in self.data_dims}\n",
    "        \n",
    "        # Use shared spacings across all dimensions\n",
    "        self.all_spacings = self.spacings_info['shared_spacings']\n",
    "        \n",
    "        # Cache values to avoid redundant computation\n",
    "        self._boot_indexes_cache = {}\n",
    "        \n",
    "        # Pre-compute indexes for all spacings\n",
    "        self._compute_all_spacing_indexes()\n",
    "\n",
    "    def _check_and_transpose_dimensions(self):\n",
    "        \"\"\"\n",
    "        Check if dimensions are in the correct order (y,x), (z,x) or (z,y)\n",
    "        and transpose if needed to ensure that velocity components match the correct dimensions.\n",
    "        Algorithm will only proceed if there are exactly 2 dimensions.\n",
    "        \"\"\"\n",
    "        # Get dimensions\n",
    "        dims = self.data_dims\n",
    "        \n",
    "        # Check for exactly 2 dimensions\n",
    "        if len(dims) != 2:\n",
    "            raise ValueError(f\"Expected exactly 2 dimensions, but got {len(dims)}: {dims}. \"\n",
    "                             f\"The structure function analysis requires 2D data.\")\n",
    "        \n",
    "        # Expected dimension pairs\n",
    "        expected_pairs = [('y', 'x'), ('z', 'x'), ('z', 'y')]\n",
    "        current_pair = tuple(dims)\n",
    "        \n",
    "        # If current pair is already one of the expected pairs, no need to transpose\n",
    "        if current_pair in expected_pairs:\n",
    "            print(f\"Dimensions {current_pair} are already in the expected order\")\n",
    "            return\n",
    "        \n",
    "        # Check if the dimensions are in reverse order\n",
    "        reversed_pair = (dims[1], dims[0])\n",
    "        if reversed_pair in expected_pairs:\n",
    "            # Transpose to correct order\n",
    "            self.ds = self.ds.transpose(dims[1], dims[0])\n",
    "            self.data_dims = list(self.ds.dims)\n",
    "            print(f\"Transposed dimensions from {dims} to {self.data_dims}\")\n",
    "            return\n",
    "        \n",
    "        # If we get here, the dimensions don't match any expected pairs even after reversing\n",
    "        raise ValueError(f\"Dimensions {dims} are not compatible with expected dimension pairs: {expected_pairs}. \"\n",
    "                         f\"Please provide data with one of the expected dimension pairs.\")\n",
    "\n",
    "    \n",
    "    def _get_simplified_adaptive_spacings(self):\n",
    "        \"\"\"\n",
    "        Calculate adaptive spacings based on dimension sizes.\n",
    "        Uses shared spacings across all dimensions based on the most limiting dimension.\n",
    "        Prints warnings if bootsize limits the range of accessible scales.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary with 'shared_spacings' for all dimensions\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        dim_ratios = {}\n",
    "        \n",
    "        # Calculate the ratios (size/bootsize) for each dimension\n",
    "        for dim in self.data_shape:\n",
    "            if dim in self.bootsize:\n",
    "                dim_ratios[dim] = self.data_shape[dim] / self.bootsize[dim]\n",
    "        \n",
    "        if not dim_ratios:\n",
    "            return {}  # No valid dimensions\n",
    "        \n",
    "        # Find the minimum ratio which will limit the maximum spacing\n",
    "        min_ratio = min(dim_ratios.values())\n",
    "        min_ratio_dim = min(dim_ratios, key=dim_ratios.get)\n",
    "        \n",
    "        # Calculate maximum power of 2 that can be used for spacing\n",
    "        max_power = int(math.floor(math.log2(min_ratio)))\n",
    "        \n",
    "        # Create spacings as powers of 2\n",
    "        shared_spacings = [1 << i for i in range(max_power + 1)]\n",
    "        \n",
    "        # Check if any dimension has a significantly higher ratio and print warning\n",
    "        max_ratio = max(dim_ratios.values())\n",
    "        max_ratio_dim = max(dim_ratios, key=dim_ratios.get)\n",
    "        \n",
    "        if max_ratio > 2 * min_ratio:\n",
    "            # Calculate the maximum power of 2 possible for the dimension with highest ratio\n",
    "            max_possible_power = int(math.floor(math.log2(max_ratio)))\n",
    "            \n",
    "            # Calculate what bootsize would be needed to reach this power\n",
    "            # We want: data_shape[min_ratio_dim] / new_bootsize ≥ 2^max_possible_power\n",
    "            # Therefore: new_bootsize ≤ data_shape[min_ratio_dim] / 2^max_possible_power\n",
    "            optimal_bootsize = self.data_shape[min_ratio_dim] / (1 << max_possible_power)\n",
    "            \n",
    "            # Find the nearest power of 2 smaller than or equal to optimal_bootsize\n",
    "            nearest_power2_bootsize = 1 << int(math.floor(math.log2(optimal_bootsize)))\n",
    "            \n",
    "            print(f\"WARNING: Dimension '{min_ratio_dim}' with bootsize {self.bootsize[min_ratio_dim]} \"\n",
    "                  f\"is limiting the range of accessible scales.\")\n",
    "            print(f\"For optimal results, consider adjusting bootsize for '{min_ratio_dim}' to {nearest_power2_bootsize} \"\n",
    "                  f\"(power of 2) to match the scale range of dimension '{max_ratio_dim}'.\")\n",
    "            print(f\"Current accessible spacings are limited to: {shared_spacings}\")\n",
    "        \n",
    "        # Store results\n",
    "        result['shared_spacings'] = shared_spacings\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _compute_all_spacing_indexes(self):\n",
    "        \"\"\"\n",
    "        Pre-compute indexes for all possible spacings to improve performance.\n",
    "        This is a one-time operation during initialization.\n",
    "        \"\"\"\n",
    "        self._spacing_indexes = {}\n",
    "        for sp_value in self.all_spacings:\n",
    "            spacing = {dim: sp_value for dim in self.data_dims}\n",
    "            self._spacing_indexes[sp_value] = self._compute_indexes_for_spacing(spacing)   \n",
    "            \n",
    "    def _compute_indexes_for_spacing(self, spacing):\n",
    "        indexes = {}\n",
    "        bootsizes = self.bootsize if isinstance(self.bootsize, dict) else dict(zip(self.data_dims, self.bootsize))\n",
    "    \n",
    "        for dim in self.data_dims:\n",
    "            sp_value = spacing[dim] if isinstance(spacing, dict) else spacing\n",
    "            indexes[dim] = sliding_window_view(np.arange(self.data_shape[dim]), \n",
    "                                          (self.data_shape[dim] - bootsizes[dim]*sp_value + 1,), \n",
    "                                          writeable=False)[::sp_value]\n",
    "        return indexes\n",
    "  \n",
    "        \n",
    "    def get_boot_indexes(self, spacing=None):\n",
    "        \"\"\"\n",
    "        Get boot indexes for a specific spacing, using cached values when available.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        spacing : dict or int, optional\n",
    "            Spacing values to use. If None, uses the instance's current spacing.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary with boot indexes for each dimension\n",
    "        \"\"\"\n",
    "        # Use current spacing if None provided\n",
    "        if spacing is None:\n",
    "            spacing = self.spacing\n",
    "        \n",
    "        # Handle different input types for spacing\n",
    "        if isinstance(spacing, dict):\n",
    "            # Check if this is one of our pre-computed spacings\n",
    "            if len(spacing) > 0 and all(sp == list(spacing.values())[0] for sp in spacing.values()):\n",
    "                sp_value = list(spacing.values())[0]\n",
    "                if sp_value in self._spacing_indexes:\n",
    "                    return self._spacing_indexes[sp_value]\n",
    "            \n",
    "            # Calculate spacing key for caching\n",
    "            spacing_key = '_'.join(f\"{dim}_{sp}\" for dim, sp in spacing.items())\n",
    "        else:\n",
    "            # Single value spacing\n",
    "            if spacing in self._spacing_indexes:\n",
    "                return self._spacing_indexes[spacing]\n",
    "            spacing_key = str(spacing)\n",
    "        \n",
    "        # Check cache\n",
    "        if spacing_key in self._boot_indexes_cache:\n",
    "            return self._boot_indexes_cache[spacing_key]\n",
    "        \n",
    "        # Compute new indexes if not in cache\n",
    "        if isinstance(spacing, dict):\n",
    "            indexes = self._compute_indexes_for_spacing(spacing)\n",
    "        else:\n",
    "            indexes = self._compute_indexes_for_spacing({dim: spacing for dim in self.data_dims})\n",
    "        \n",
    "        # Cache result\n",
    "        self._boot_indexes_cache[spacing_key] = indexes\n",
    "        return indexes\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "                \n",
    "    def set_spacing(self, spacing):\n",
    "        \"\"\"\n",
    "        Set the current spacing to be used for calculations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        spacing : dict or int\n",
    "            Spacing values to use. Can be a single integer for the same spacing in all dimensions,\n",
    "            or a dictionary mapping dimensions to their specific spacing values.\n",
    "        \"\"\"\n",
    "        if isinstance(spacing, int):\n",
    "            spacing = {dim: spacing for dim in self.data_dims}\n",
    "        self.spacing = spacing\n",
    "        \n",
    "    def get_all_spacings(self):\n",
    "        \"\"\"\n",
    "        Get all possible spacings that have been pre-computed.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of all possible spacing values\n",
    "        \"\"\"\n",
    "        return self.all_spacings\n",
    "\n",
    "\n",
    "    def _check_and_reorder_variables(self, variables_names, dims, fun='longitudinal'):\n",
    "        \"\"\"\n",
    "        Check if the provided variable names match the expected components for the given plane and function type,\n",
    "        and reorder them if necessary.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        variables_names : list\n",
    "            List of variable names provided by the user\n",
    "        dims : list\n",
    "            List of dimension names (e.g., ['y', 'x'])\n",
    "        fun : str\n",
    "            Type of structure function\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            Tuple of variable names in the correct order for the given plane and function type\n",
    "        \"\"\"\n",
    "        # Expected velocity component mappings for each plane\n",
    "        velocity_vars = {\n",
    "            ('y', 'x'): ['u', 'v'],  # (y, x) plane expects u, v components\n",
    "            ('z', 'x'): ['u', 'w'],  # (z, x) plane expects u, w components\n",
    "            ('z', 'y'): ['v', 'w']   # (z, y) plane expects v, w components\n",
    "        }\n",
    "        \n",
    "        # Get the expected variables based on function type and plane\n",
    "        plane_tuple = tuple(dims)\n",
    "        if plane_tuple not in velocity_vars:\n",
    "            raise ValueError(f\"Unsupported dimension combination: {dims}\")\n",
    "        \n",
    "        expected_vel = velocity_vars[plane_tuple]\n",
    "        provided = list(variables_names)\n",
    "        \n",
    "        # Handle different function types\n",
    "        if fun in ['longitudinal', 'transverse', 'default_vel', 'longitudinal_transverse']:\n",
    "            # These functions need exactly 2 velocity components\n",
    "            if len(provided) != 2:\n",
    "                raise ValueError(f\"{fun} structure function requires exactly 2 velocity components, got {len(provided)}\")\n",
    "            \n",
    "            # Check if variables match expected velocity components (in any order)\n",
    "            if set(provided) == set(expected_vel):\n",
    "                # Variables match, but might be in wrong order\n",
    "                if provided != expected_vel:\n",
    "                    print(f\"Reordering variables from {provided} to {expected_vel} to match {plane_tuple} plane\")\n",
    "                    return tuple(expected_vel)\n",
    "                return tuple(provided)\n",
    "            \n",
    "            # Try to map provided variables to expected ones using pattern matching\n",
    "            mapped_vars = self._map_variables_by_pattern(provided, expected_vel, plane_tuple)\n",
    "            if mapped_vars:\n",
    "                return mapped_vars\n",
    "        \n",
    "        elif fun == 'scalar':\n",
    "            # Scalar function needs exactly 1 scalar variable\n",
    "            if len(provided) != 1:\n",
    "                raise ValueError(f\"Scalar structure function requires exactly 1 scalar variable, got {len(provided)}\")\n",
    "            \n",
    "            # No reordering needed for single scalar\n",
    "            return tuple(provided)\n",
    "        \n",
    "        elif fun == 'scalar_scalar':\n",
    "            # Scalar-scalar function needs exactly 2 scalar variables\n",
    "            if len(provided) != 2:\n",
    "                raise ValueError(f\"Scalar-scalar structure function requires exactly 2 scalar variables, got {len(provided)}\")\n",
    "            \n",
    "            # No specific ordering required for scalar-scalar\n",
    "            return tuple(provided)\n",
    "        \n",
    "        elif fun in ['longitudinal_scalar', 'transverse_scalar']:\n",
    "            # These functions need 2 velocity components and 1 scalar\n",
    "            if len(provided) != 3:\n",
    "                raise ValueError(f\"{fun} structure function requires 2 velocity components and 1 scalar, got {len(provided)}\")\n",
    "            \n",
    "            # Try to identify which are velocity components and which is the scalar\n",
    "            vel_candidates = []\n",
    "            scalar_candidates = []\n",
    "            \n",
    "            for var in provided:\n",
    "                if any(vel_pattern in var.lower() for vel_pattern in ['u', 'v', 'w', 'vel', 'velocity']):\n",
    "                    vel_candidates.append(var)\n",
    "                else:\n",
    "                    scalar_candidates.append(var)\n",
    "            \n",
    "            # If we can't clearly distinguish, assume the first two are velocity and the last is scalar\n",
    "            if len(vel_candidates) != 2 or len(scalar_candidates) != 1:\n",
    "                print(f\"Warning: Could not clearly distinguish velocity components from scalar in {provided}. \"\n",
    "                      f\"Assuming the first two are velocity components and the last is the scalar.\")\n",
    "                vel_candidates = provided[:2]\n",
    "                scalar_candidates = [provided[2]]\n",
    "            \n",
    "            # Check and reorder velocity components\n",
    "            mapped_vel = self._map_variables_by_pattern(vel_candidates, expected_vel, plane_tuple)\n",
    "            if mapped_vel:\n",
    "                # Return velocity components first, then scalar\n",
    "                return tuple(list(mapped_vel) + scalar_candidates)\n",
    "        \n",
    "        # If we get here, something went wrong with the mapping\n",
    "        raise ValueError(f\"Failed to properly map variables {provided} for {fun} structure function on {plane_tuple} plane.\")\n",
    "    \n",
    "    def _map_variables_by_pattern(self, provided, expected, plane_tuple):\n",
    "        \"\"\"\n",
    "        Map provided variables to expected ones using common naming patterns.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        provided : list\n",
    "            List of provided variable names\n",
    "        expected : list\n",
    "            List of expected variable names\n",
    "        plane_tuple : tuple\n",
    "            Tuple of dimension names\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple or None\n",
    "            Tuple of mapped variable names or None if mapping fails\n",
    "        \"\"\"\n",
    "        # Common naming patterns for velocity components\n",
    "        var_patterns = {\n",
    "            'u': ['u', 'u_vel', 'velocity_x', 'vx', 'vel_x'],\n",
    "            'v': ['v', 'v_vel', 'velocity_y', 'vy', 'vel_y'],\n",
    "            'w': ['w', 'w_vel', 'velocity_z', 'vz', 'vel_z']\n",
    "        }\n",
    "        \n",
    "        # Try to map provided variables to expected ones\n",
    "        mapped_vars = []\n",
    "        for expected_var in expected:\n",
    "            patterns = var_patterns[expected_var]\n",
    "            matching_vars = [v for v in provided if any(pattern in v.lower() for pattern in patterns)]\n",
    "            \n",
    "            if matching_vars:\n",
    "                mapped_vars.append(matching_vars[0])\n",
    "            else:\n",
    "                # No matching variable found for this expected component\n",
    "                return None\n",
    "        \n",
    "        if len(mapped_vars) == len(expected):\n",
    "            print(f\"Mapped variables {provided} to {mapped_vars} for {plane_tuple} plane\")\n",
    "            return tuple(mapped_vars)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _calc_longitudinal(self, subset, variables_names, order, dims, ny, nx, shift_cache):\n",
    "        \"\"\"\n",
    "        Calculate longitudinal structure function: (du*dx + dv*dy)^n / |r|^n\n",
    "        or (du*dx + dw*dz)^n / |r|^n or (dv*dy + dw*dz)^n / |r|^n depending on the plane.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        subset : xarray.Dataset\n",
    "            Subset of the dataset containing required variables\n",
    "        variables_names : list\n",
    "            List of variable names (should contain two velocity components)\n",
    "        order : int\n",
    "            Order of the structure function\n",
    "        dims, ny, nx, shift_cache : various\n",
    "            Additional parameters needed for calculation\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray, numpy.ndarray, numpy.ndarray\n",
    "            Structure function values, DX values, DY values\n",
    "        \"\"\"\n",
    "        if len(variables_names) != 2:\n",
    "            raise ValueError(f\"Longitudinal structure function requires exactly 2 velocity components, got {len(variables_names)}\")\n",
    "        \n",
    "        # Check and reorder variables if needed based on plane\n",
    "        var1, var2 = self._check_and_reorder_variables(variables_names, dims)\n",
    "        \n",
    "        # Arrays to store results\n",
    "        results = np.full(ny * nx, np.nan)\n",
    "        dx_vals = np.full(ny * nx, 0.0)\n",
    "        dy_vals = np.full(ny * nx, 0.0)\n",
    "        \n",
    "        # Get the velocity components\n",
    "        comp1_var = subset[var1].values\n",
    "        comp2_var = subset[var2].values\n",
    "        \n",
    "        # Get coordinate variables based on the plane\n",
    "        if dims == ['y', 'x']:\n",
    "            # (y, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.y.values\n",
    "        elif dims == ['z', 'x']:\n",
    "            # (z, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.z.values  # Using y_coord to store z-coordinate for consistency\n",
    "        elif dims == ['z', 'y']:\n",
    "            # (z, y) plane\n",
    "            x_coord = subset.y.values  # Using x_coord to store y-coordinate for consistency\n",
    "            y_coord = subset.z.values\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dimension combination: {dims}\")\n",
    "        \n",
    "        # Loop through all points\n",
    "        idx = 0\n",
    "        for iy in range(ny):\n",
    "            for ix in range(nx):\n",
    "                                \n",
    "                # Compute actual physical separation\n",
    "                dx = fast_shift(x_coord, iy,ix) - x_coord\n",
    "                dy = fast_shift(y_coord, iy,ix) - y_coord\n",
    "                \n",
    "                \n",
    "                # Compute norm of separation vector\n",
    "                norm = np.maximum(np.sqrt(dx**2 + dy**2),1e-10) # to avoid dividing by zero\n",
    "                 \n",
    "                \n",
    "                # Calculate velocity differences\n",
    "                dcomp1 = fast_shift(comp1_var, iy,ix) - comp1_var\n",
    "                dcomp2 = fast_shift(comp2_var, iy,ix) - comp2_var\n",
    "                \n",
    "                # Store the separation distances\n",
    "                dx_vals[idx] = bn.nanmean(dx)\n",
    "                dy_vals[idx] = bn.nanmean(dy)\n",
    "                \n",
    "                \n",
    "                # Project velocity difference onto separation direction (longitudinal)\n",
    "                delta_parallel = dcomp1 * (dx/norm) + dcomp2 * (dy/norm)\n",
    "                \n",
    "                # Compute structure function\n",
    "                sf_val = (delta_parallel) ** order\n",
    "                results[idx] = bn.nanmean(sf_val)\n",
    "                \n",
    "                idx += 1\n",
    "                \n",
    "        return results, dx_vals, dy_vals\n",
    "\n",
    "    def _calc_transverse(self, subset, variables_names, order, dims, ny, nx, shift_cache):\n",
    "        \"\"\"\n",
    "        Calculate transverse structure function: (du*dy - dv*dx)^n / |r|^n\n",
    "        or (du*dz - dw*dx)^n / |r|^n or (dv*dz - dw*dy)^n / |r|^n depending on the plane.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        subset : xarray.Dataset\n",
    "            Subset of the dataset containing required variables\n",
    "        variables_names : list\n",
    "            List of variable names (should contain two velocity components)\n",
    "        order : int\n",
    "            Order of the structure function\n",
    "        dims, ny, nx, shift_cache : various\n",
    "            Additional parameters needed for calculation\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray, numpy.ndarray, numpy.ndarray\n",
    "            Structure function values, DX values, DY values\n",
    "        \"\"\"\n",
    "        if len(variables_names) != 2:\n",
    "            raise ValueError(f\"Transverse structure function requires exactly 2 velocity components, got {len(variables_names)}\")\n",
    "        \n",
    "        # Check and reorder variables if needed based on plane\n",
    "        var1, var2 = self._check_and_reorder_variables(variables_names, dims, fun='transverse')\n",
    "        \n",
    "        # Arrays to store results\n",
    "        results = np.full(ny * nx, np.nan)\n",
    "        dx_vals = np.full(ny * nx, 0.0)\n",
    "        dy_vals = np.full(ny * nx, 0.0)\n",
    "        \n",
    "        # Get the velocity components\n",
    "        comp1_var = subset[var1].values\n",
    "        comp2_var = subset[var2].values\n",
    "        \n",
    "        # Get coordinate variables based on the plane\n",
    "        if dims == ['y', 'x']:\n",
    "            # (y, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.y.values\n",
    "        elif dims == ['z', 'x']:\n",
    "            # (z, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.z.values  # Using y_coord to store z-coordinate for consistency\n",
    "        elif dims == ['z', 'y']:\n",
    "            # (z, y) plane\n",
    "            x_coord = subset.y.values  # Using x_coord to store y-coordinate for consistency\n",
    "            y_coord = subset.z.values\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dimension combination: {dims}\")\n",
    "        \n",
    "        # Loop through all points\n",
    "        idx = 0\n",
    "        for iy in range(ny):\n",
    "            for ix in range(nx):\n",
    "                                \n",
    "                # Compute actual physical separation\n",
    "                dx = fast_shift(x_coord, iy, ix) - x_coord\n",
    "                dy = fast_shift(y_coord, iy, ix) - y_coord\n",
    "                \n",
    "                \n",
    "                # Compute norm of separation vector\n",
    "                norm = np.maximum(np.sqrt(dx**2 + dy**2),1.0e-10)                \n",
    "                \n",
    "                # Calculate velocity differences\n",
    "                dcomp1 = fast_shift(comp1_var, iy, ix) - comp1_var\n",
    "                dcomp2 = fast_shift(comp2_var, iy, ix) - comp2_var\n",
    "                \n",
    "                # Store the separation distances\n",
    "                dx_vals[idx] = bn.nanmean(dx)\n",
    "                dy_vals[idx] = bn.nanmean(dy)\n",
    "                \n",
    "                # Calculate transverse component (perpendicular to separation direction)\n",
    "\n",
    "                delta_perp = dcomp1 * (dy/norm) - dcomp2 * (dx/norm)\n",
    "                \n",
    "                # Compute structure function\n",
    "                sf_val = (delta_perp) ** order\n",
    "                results[idx] = bn.nanmean(sf_val)\n",
    "                \n",
    "                idx += 1\n",
    "                \n",
    "        return results, dx_vals, dy_vals\n",
    "\n",
    "    def _calc_default_vel(self, subset, variables_names, order, dims, ny, nx, shift_cache):\n",
    "        \"\"\"\n",
    "        Calculate default velocity structure function: (du^n + dv^n)\n",
    "        or (du^n + dw^n) or (dv^n + dw^n) depending on the plane.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        subset : xarray.Dataset\n",
    "            Subset of the dataset containing required variables\n",
    "        variables_names : list\n",
    "            List of variable names (should contain two velocity components)\n",
    "        order : int\n",
    "            Order of the structure function\n",
    "        dims, ny, nx, shift_cache : various\n",
    "            Additional parameters needed for calculation\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray, numpy.ndarray, numpy.ndarray\n",
    "            Structure function values, DX values, DY values\n",
    "        \"\"\"\n",
    "        if len(variables_names) != 2:\n",
    "            raise ValueError(f\"Default velocity structure function requires exactly 2 velocity components, got {len(variables_names)}\")\n",
    "        \n",
    "        # Check and reorder variables if needed based on plane\n",
    "        var1, var2 = self._check_and_reorder_variables(variables_names, dims, fun='default_vel')\n",
    "        \n",
    "        # Arrays to store results\n",
    "        results = np.full(ny * nx, np.nan)\n",
    "        dx_vals = np.full(ny * nx, 0.0)\n",
    "        dy_vals = np.full(ny * nx, 0.0)\n",
    "        \n",
    "        # Get the velocity components\n",
    "        comp1_var = subset[var1].values\n",
    "        comp2_var = subset[var2].values\n",
    "        \n",
    "        # Get coordinate variables based on the plane\n",
    "        if dims == ['y', 'x']:\n",
    "            # (y, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.y.values\n",
    "        elif dims == ['z', 'x']:\n",
    "            # (z, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.z.values  # Using y_coord to store z-coordinate for consistency\n",
    "        elif dims == ['z', 'y']:\n",
    "            # (z, y) plane\n",
    "            x_coord = subset.y.values  # Using x_coord to store y-coordinate for consistency\n",
    "            y_coord = subset.z.values\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dimension combination: {dims}\")\n",
    "        \n",
    "        # Loop through all points\n",
    "        idx = 0\n",
    "        for iy in range(ny):\n",
    "            for ix in range(nx):\n",
    "\n",
    "                \n",
    "                # Compute coordinate shifts\n",
    "                x_shifted = fast_shift(x_coord, shift_key[0], shift_key[1])\n",
    "                y_shifted = fast_shift(y_coord, shift_key[0], shift_key[1])\n",
    "                \n",
    "                # Compute actual physical separation\n",
    "                dx = fast_shift(x_coord, iy, ix) - x_coord\n",
    "                dy = fast_shift(y_coord, iy, ix) - y_coord\n",
    "                                \n",
    "                # Calculate velocity differences\n",
    "                dcomp1 = fast_shift(comp1_var, iy, ix) - comp1_var\n",
    "                dcomp2 = fast_shift(comp2_var, iy, ix) - comp2_var\n",
    "                \n",
    "                # Store the separation distances\n",
    "                dx_vals[idx] = bn.nanmean(dx)\n",
    "                dy_vals[idx] = bn.nanmean(dy)\n",
    "                \n",
    "                # Calculate default velocity structure function: du^n + dv^n\n",
    "                sf_val = (dcomp1 ** order) + (dcomp2 ** order)\n",
    "                results[idx] = bn.nanmean(sf_val)\n",
    "                \n",
    "                idx += 1\n",
    "                \n",
    "        return results, dx_vals, dy_vals\n",
    "\n",
    "    def _calc_scalar(self, subset, variables_names, order, dims, ny, nx, shift_cache):\n",
    "        \"\"\"\n",
    "        Calculate scalar structure function: (dscalar^n)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        subset : xarray.Dataset\n",
    "            Subset of the dataset containing required variables\n",
    "        variables_names : list\n",
    "            List of variable names (should contain one scalar variable)\n",
    "        order : int\n",
    "            Order of the structure function\n",
    "        dims, ny, nx, shift_cache : various\n",
    "            Additional parameters needed for calculation\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray, numpy.ndarray, numpy.ndarray\n",
    "            Structure function values, DX values, DY values\n",
    "        \"\"\"\n",
    "        if len(variables_names) != 1:\n",
    "            raise ValueError(f\"Scalar structure function requires exactly 1 scalar variable, got {len(variables_names)}\")\n",
    "        \n",
    "        # Get the scalar variable name\n",
    "        scalar_name = variables_names[0]\n",
    "        \n",
    "        # Arrays to store results\n",
    "        results = np.full(ny * nx, np.nan)\n",
    "        dx_vals = np.full(ny * nx, 0.0)\n",
    "        dy_vals = np.full(ny * nx, 0.0)\n",
    "        \n",
    "        # Get the scalar variable\n",
    "        scalar_var = subset[scalar_name].values\n",
    "        \n",
    "        # Get coordinate variables based on the plane\n",
    "        if dims == ['y', 'x']:\n",
    "            # (y, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.y.values\n",
    "        elif dims == ['z', 'x']:\n",
    "            # (z, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.z.values  # Using y_coord to store z-coordinate for consistency\n",
    "        elif dims == ['z', 'y']:\n",
    "            # (z, y) plane\n",
    "            x_coord = subset.y.values  # Using x_coord to store y-coordinate for consistency\n",
    "            y_coord = subset.z.values\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dimension combination: {dims}\")\n",
    "        \n",
    "        # Loop through all points\n",
    "        idx = 0\n",
    "        for iy in range(ny):\n",
    "            for ix in range(nx):\n",
    "\n",
    "                \n",
    "                # Compute actual physical separation\n",
    "                dx = fast_shift(x_coord, iy, ix) - x_coord\n",
    "                dy = fast_shift(y_coord, iy, ix) - y_coord\n",
    "                \n",
    "                # Calculate scalar difference\n",
    "                dscalar = fast_shift(scalar_var, iy, ix) - scalar_var\n",
    "                \n",
    "                # Store the separation distances\n",
    "                dx_vals[idx] = bn.nanmean(dx)\n",
    "                dy_vals[idx] = bn.nanmean(dy)\n",
    "                \n",
    "                # Calculate scalar structure function: dscalar^n\n",
    "                sf_val = dscalar ** order\n",
    "                results[idx] = bn.nanmean(sf_val)\n",
    "                \n",
    "                idx += 1\n",
    "                \n",
    "        return results, dx_vals, dy_vals\n",
    "\n",
    "    def _calc_scalar_scalar(self, subset, variables_names, order, dims, ny, nx, shift_cache):\n",
    "        \"\"\"\n",
    "        Calculate scalar-scalar structure function: (dscalar1^n * dscalar2^k)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        subset : xarray.Dataset\n",
    "            Subset of the dataset containing required variables\n",
    "        variables_names : list\n",
    "            List of variable names (should contain two scalar variables)\n",
    "        order : tuple\n",
    "            Tuple of orders (n, k) for the structure function\n",
    "        dims, ny, nx, shift_cache : various\n",
    "            Additional parameters needed for calculation\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray, numpy.ndarray, numpy.ndarray\n",
    "            Structure function values, DX values, DY values\n",
    "        \"\"\"\n",
    "        if len(variables_names) != 2:\n",
    "            raise ValueError(f\"Scalar-scalar structure function requires exactly 2 scalar components, got {len(variables_names)}\")\n",
    "        \n",
    "        if not isinstance(order, tuple) or len(order) != 2:\n",
    "            raise ValueError(f\"Order must be a tuple (n, k) for scalar-scalar structure function, got {order}\")\n",
    "        \n",
    "        # Unpack order tuple\n",
    "        n, k = order\n",
    "        \n",
    "        # Check and reorder variables if needed based on plane\n",
    "        var1, var2 = variables_names\n",
    "\n",
    "        \n",
    "        # Arrays to store results\n",
    "        results = np.full(ny * nx, np.nan)\n",
    "        dx_vals = np.full(ny * nx, 0.0)\n",
    "        dy_vals = np.full(ny * nx, 0.0)\n",
    "        \n",
    "        # Get the scalar variable\n",
    "        scalar_var1 = subset[var1].values\n",
    "        scalar_var2 = subset[var2].values\n",
    "        \n",
    "        # Get coordinate variables based on the plane\n",
    "        if dims == ['y', 'x']:\n",
    "            # (y, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.y.values\n",
    "        elif dims == ['z', 'x']:\n",
    "            # (z, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.z.values  # Using y_coord to store z-coordinate for consistency\n",
    "        elif dims == ['z', 'y']:\n",
    "            # (z, y) plane\n",
    "            x_coord = subset.y.values  # Using x_coord to store y-coordinate for consistency\n",
    "            y_coord = subset.z.values\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dimension combination: {dims}\")\n",
    "        \n",
    "        # Loop through all points\n",
    "        idx = 0\n",
    "        for iy in range(ny):\n",
    "            for ix in range(nx):\n",
    "\n",
    "                \n",
    "                # Compute actual physical separation\n",
    "                dx = fast_shift(x_coord, iy, ix) - x_coord\n",
    "                dy = fast_shift(y_coord, iy, ix) - y_coord\n",
    "                \n",
    "                # Calculate scalars difference\n",
    "                dscalar1 = fast_shift(scalar_var1, iy, ix) - scalar_var1\n",
    "                dscalar2 = fast_shift(scalar_var2, iy, ix) - scalar_var2\n",
    "                \n",
    "                # Store the separation distances\n",
    "                dx_vals[idx] = bn.nanmean(dx)\n",
    "                dy_vals[idx] = bn.nanmean(dy)\n",
    "                \n",
    "                # Calculate scalar-scalar structure function: dscalar^n * dscalar^k\n",
    "                sf_val = (dscalar1 ** n) * (dscalar2 ** k)\n",
    "                results[idx] = bn.nanmean(sf_val)\n",
    "                \n",
    "                idx += 1\n",
    "                \n",
    "        return results, dx_vals, dy_vals\n",
    "        \n",
    "    def _calc_longitudinal_transverse(self, subset, variables_names, order, dims, ny, nx, shift_cache):\n",
    "        \"\"\"\n",
    "        Calculate cross longitudinal-transverse structure function: (du_longitudinal^n * du_transverse^k)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        subset : xarray.Dataset\n",
    "            Subset of the dataset containing required variables\n",
    "        variables_names : list\n",
    "            List of variable names (should contain two velocity components)\n",
    "        order : tuple\n",
    "            Tuple of orders (n, k) for the structure function\n",
    "        dims, ny, nx, shift_cache : various\n",
    "            Additional parameters needed for calculation\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray, numpy.ndarray, numpy.ndarray\n",
    "            Structure function values, DX values, DY values\n",
    "        \"\"\"\n",
    "        if len(variables_names) != 2:\n",
    "            raise ValueError(f\"Longitudinal-transverse structure function requires exactly 2 velocity components, got {len(variables_names)}\")\n",
    "        \n",
    "        if not isinstance(order, tuple) or len(order) != 2:\n",
    "            raise ValueError(f\"Order must be a tuple (n, k) for longitudinal-transverse structure function, got {order}\")\n",
    "        \n",
    "        # Unpack order tuple\n",
    "        n, k = order\n",
    "        \n",
    "        # Check and reorder variables if needed based on plane\n",
    "        var1, var2 = self._check_and_reorder_variables(variables_names, dims, fun='longitudinal_transverse')\n",
    "        \n",
    "        # Arrays to store results\n",
    "        results = np.full(ny * nx, np.nan)\n",
    "        dx_vals = np.full(ny * nx, 0.0)\n",
    "        dy_vals = np.full(ny * nx, 0.0)\n",
    "        \n",
    "        # Get the velocity components\n",
    "        comp1_var = subset[var1].values\n",
    "        comp2_var = subset[var2].values\n",
    "        \n",
    "        # Get coordinate variables based on the plane\n",
    "        if dims == ['y', 'x']:\n",
    "            # (y, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.y.values\n",
    "        elif dims == ['z', 'x']:\n",
    "            # (z, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.z.values  # Using y_coord to store z-coordinate for consistency\n",
    "        elif dims == ['z', 'y']:\n",
    "            # (z, y) plane\n",
    "            x_coord = subset.y.values  # Using x_coord to store y-coordinate for consistency\n",
    "            y_coord = subset.z.values\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dimension combination: {dims}\")\n",
    "        \n",
    "        # Loop through all points\n",
    "        idx = 0\n",
    "        for iy in range(ny):\n",
    "            for ix in range(nx):\n",
    "\n",
    "                \n",
    "                # Compute actual physical separation\n",
    "                dx = fast_shift(x_coord, iy, ix) - x_coord\n",
    "                dy = fast_shift(y_coord, iy, ix) - y_coord\n",
    "                \n",
    "                \n",
    "                # Compute norm of separation vector\n",
    "                norm = np.maximum(np.sqrt(dx**2 + dy**2),1.0e-10)\n",
    "                                \n",
    "                # Calculate velocity differences\n",
    "                dcomp1 = fast_shift(comp1_var, iy, ix) - comp1_var\n",
    "                dcomp2 = fast_shift(comp2_var, iy, ix) - comp2_var\n",
    "                \n",
    "                # Store the separation distances\n",
    "                dx_vals[idx] = bn.nanmean(dx)\n",
    "                dy_vals[idx] = bn.nanmean(dy)\n",
    "                \n",
    "                \n",
    "                # Project velocity difference onto separation direction (longitudinal)\n",
    "                delta_parallel = dcomp1 * (dx/norm) + dcomp2 * (dy/norm)\n",
    "                \n",
    "                # Calculate perpendicular component (transverse)\n",
    "                delta_perp = dcomp1 * (dy/norm) - dcomp2 * (dx/norm)\n",
    "                \n",
    "                # Calculate longitudinal-transverse structure function: delta_parallel^n * delta_perp^k\n",
    "                sf_val = (delta_parallel ** n) * (delta_perp ** k)\n",
    "                results[idx] = bn.nanmean(sf_val)\n",
    "                \n",
    "                idx += 1\n",
    "                \n",
    "        return results, dx_vals, dy_vals\n",
    "\n",
    "\n",
    "    def _calc_longitudinal_scalar(self, subset, variables_names, order, dims, ny, nx, shift_cache):\n",
    "        \"\"\"\n",
    "        Calculate cross longitudinal-scalar structure function: (du_longitudinal^n * dscalar^k)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        subset : xarray.Dataset\n",
    "            Subset of the dataset containing required variables\n",
    "        variables_names : list\n",
    "            List of variable names (should contain two velocity components and one scalar)\n",
    "        order : tuple\n",
    "            Tuple of orders (n, k) for the structure function\n",
    "        dims, ny, nx, shift_cache : various\n",
    "            Additional parameters needed for calculation\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray, numpy.ndarray, numpy.ndarray\n",
    "            Structure function values, DX values, DY values\n",
    "        \"\"\"\n",
    "        if len(variables_names) != 3:\n",
    "            raise ValueError(f\"Longitudinal-scalar structure function requires 3 variables (2 velocity components and 1 scalar), got {len(variables_names)}\")\n",
    "        \n",
    "        if not isinstance(order, tuple) or len(order) != 2:\n",
    "            raise ValueError(f\"Order must be a tuple (n, k) for longitudinal-scalar structure function, got {order}\")\n",
    "        \n",
    "        # Unpack order tuple\n",
    "        n, k = order\n",
    "        \n",
    "        # Check and reorder variables if needed based on plane\n",
    "        vel_vars, scalar_var = self._check_and_reorder_variables(variables_names, dims, fun='longitudinal_scalar')\n",
    "        var1, var2 = vel_vars\n",
    "        \n",
    "        # Arrays to store results\n",
    "        results = np.full(ny * nx, np.nan)\n",
    "        dx_vals = np.full(ny * nx, 0.0)\n",
    "        dy_vals = np.full(ny * nx, 0.0)\n",
    "        \n",
    "        # Get the velocity components and scalar\n",
    "        comp1_var = subset[var1].values\n",
    "        comp2_var = subset[var2].values\n",
    "        scalar_var_values = subset[scalar_var].values\n",
    "        \n",
    "        # Get coordinate variables based on the plane\n",
    "        if dims == ['y', 'x']:\n",
    "            # (y, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.y.values\n",
    "            print(f\"Using (y, x) plane with components {var1}, {var2} and scalar {scalar_var}\")\n",
    "        elif dims == ['z', 'x']:\n",
    "            # (z, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.z.values  # Using y_coord to store z-coordinate for consistency\n",
    "            print(f\"Using (z, x) plane with components {var1}, {var2} and scalar {scalar_var}\")\n",
    "        elif dims == ['z', 'y']:\n",
    "            # (z, y) plane\n",
    "            x_coord = subset.y.values  # Using x_coord to store y-coordinate for consistency\n",
    "            y_coord = subset.z.values\n",
    "            print(f\"Using (z, y) plane with components {var1}, {var2} and scalar {scalar_var}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dimension combination: {dims}\")\n",
    "        \n",
    "        # Loop through all points\n",
    "        idx = 0\n",
    "        for iy in range(ny):\n",
    "            for ix in range(nx):\n",
    "                \n",
    "                # Compute actual physical separation\n",
    "                dx = fast_shift(x_coord, iy, ix) - x_coord\n",
    "                dy = fast_shift(y_coord, iy, ix) - y_coord\n",
    "                \n",
    "                \n",
    "                # Compute norm of separation vector\n",
    "                norm = np.maximum(np.sqrt(dx**2 + dy**2),1.0e-10)\n",
    "                \n",
    "                # Calculate velocity and scalar differences\n",
    "                dcomp1 = fast_shift(comp1_var, iy, ix) - comp1_var\n",
    "                dcomp2 = fast_shift(comp2_var, iy, ix) - comp2_var\n",
    "                dscalar = fast_shift(scalar_var_values, iy, ix) - scalar_var_values\n",
    "                \n",
    "                # Store the separation distances\n",
    "                dx_vals[idx] = bn.nanmean(dx)\n",
    "                dy_vals[idx] = bn.nanmean(dy)\n",
    "                \n",
    "                \n",
    "                # Project velocity difference onto separation direction (longitudinal)\n",
    "                delta_parallel = dcomp1 * (dx/norm) + dcomp2 * (dy/norm)\n",
    "                \n",
    "                # Calculate longitudinal-scalar structure function: delta_parallel^n * dscalar^k\n",
    "                sf_val = (delta_parallel ** n) * (dscalar ** k)\n",
    "                results[idx] = bn.nanmean(sf_val)\n",
    "                \n",
    "                idx += 1\n",
    "                \n",
    "        return results, dx_vals, dy_vals\n",
    "\n",
    "\n",
    "    def _calc_transverse_scalar(self, subset, variables_names, order, dims, ny, nx, shift_cache):\n",
    "        \"\"\"\n",
    "        Calculate cross transverse-scalar structure function: (du_transverse^n * dscalar^k)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        subset : xarray.Dataset\n",
    "            Subset of the dataset containing required variables\n",
    "        variables_names : list\n",
    "            List of variable names (should contain two velocity components and one scalar)\n",
    "        order : tuple\n",
    "            Tuple of orders (n, k) for the structure function\n",
    "        dims, ny, nx, shift_cache : various\n",
    "            Additional parameters needed for calculation\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray, numpy.ndarray, numpy.ndarray\n",
    "            Structure function values, DX values, DY values\n",
    "        \"\"\"\n",
    "        if len(variables_names) != 3:\n",
    "            raise ValueError(f\"Transverse-scalar structure function requires 3 variables (2 velocity components and 1 scalar), got {len(variables_names)}\")\n",
    "        \n",
    "        if not isinstance(order, tuple) or len(order) != 2:\n",
    "            raise ValueError(f\"Order must be a tuple (n, k) for transverse-scalar structure function, got {order}\")\n",
    "        \n",
    "        # Unpack order tuple\n",
    "        n, k = order\n",
    "        \n",
    "        # Check and reorder variables if needed based on plane\n",
    "        vel_vars, scalar_var = self._check_and_reorder_variables(variables_names, dims, fun='transverse_scalar')\n",
    "        var1, var2 = vel_vars\n",
    "        \n",
    "        # Arrays to store results\n",
    "        results = np.full(ny * nx, np.nan)\n",
    "        dx_vals = np.full(ny * nx, 0.0)\n",
    "        dy_vals = np.full(ny * nx, 0.0)\n",
    "        \n",
    "        # Get the velocity components and scalar\n",
    "        comp1_var = subset[var1].values\n",
    "        comp2_var = subset[var2].values\n",
    "        scalar_var_values = subset[scalar_var].values\n",
    "        \n",
    "        # Get coordinate variables based on the plane\n",
    "        if dims == ['y', 'x']:\n",
    "            # (y, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.y.values\n",
    "            print(f\"Using (y, x) plane with components {var1}, {var2} and scalar {scalar_var}\")\n",
    "        elif dims == ['z', 'x']:\n",
    "            # (z, x) plane\n",
    "            x_coord = subset.x.values\n",
    "            y_coord = subset.z.values  # Using y_coord to store z-coordinate for consistency\n",
    "            print(f\"Using (z, x) plane with components {var1}, {var2} and scalar {scalar_var}\")\n",
    "        elif dims == ['z', 'y']:\n",
    "            # (z, y) plane\n",
    "            x_coord = subset.y.values  # Using x_coord to store y-coordinate for consistency\n",
    "            y_coord = subset.z.values\n",
    "            print(f\"Using (z, y) plane with components {var1}, {var2} and scalar {scalar_var}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dimension combination: {dims}\")\n",
    "        \n",
    "        # Loop through all points\n",
    "        idx = 0\n",
    "        for iy in range(ny):\n",
    "            for ix in range(nx):\n",
    "                \n",
    "                # Compute actual physical separation\n",
    "                dx = fast_shift(x_coord, iy, ix) - x_coord\n",
    "                dy = fast_shift(y_coord, iy, ix) - y_coord\n",
    "                \n",
    "                \n",
    "                # Compute norm of separation vector\n",
    "                norm = np.maximum(np.sqrt(dx**2 + dy**2),1.0e-10)\n",
    "                \n",
    "                \n",
    "                # Calculate velocity and scalar differences\n",
    "                dcomp1 = fast_shift(comp1_var, iy, ix) - comp1_var\n",
    "                dcomp2 = fast_shift(comp2_var, iy, ix) - comp2_var\n",
    "                dscalar = fast_shift(scalar_var_values, iy, ix) - scalar_var_values\n",
    "                \n",
    "                # Store the separation distances\n",
    "                dx_vals[idx] = bn.nanmean(dx)\n",
    "                dy_vals[idx] = bn.nanmean(dy)\n",
    "                \n",
    "                # Calculate transverse component (perpendicular to separation direction)\n",
    "                delta_perp = dcomp1 * (dy/norm) - dcomp2 * (dx/norm)\n",
    "                \n",
    "                # Calculate transverse-scalar structure function: delta_perp^n * dscalar^k\n",
    "                sf_val = (delta_perp ** n) * (dscalar ** k)\n",
    "                results[idx] = bn.nanmean(sf_val)\n",
    "                \n",
    "                idx += 1\n",
    "                \n",
    "        return results, dx_vals, dy_vals\n",
    "\n",
    "    def calculate_structure_function(self, variables_names, order, fun='longitudinal', nbx=0, nby=0, spacing=None):\n",
    "        \"\"\"\n",
    "        Main method to calculate structure functions based on specified type.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        variables_names : list\n",
    "            List of variable names to use, depends on function type\n",
    "        order : int or tuple\n",
    "            Order(s) of the structure function\n",
    "        fun : str\n",
    "            Type of structure function: ['longitudinal','transverse','default_vel','scalar',\n",
    "            'scalar_scalar','longitudinal_transverse','longitudinal_scalar','transverse_scalar']\n",
    "        nbx, nby : int\n",
    "            Bootstrap indices for x and y dimensions\n",
    "        spacing : dict or int, optional\n",
    "            Spacing values to use. If None, uses the instance's current spacing.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Structure function values\n",
    "        numpy.ndarray\n",
    "            DX values\n",
    "        numpy.ndarray\n",
    "            DY values\n",
    "        \"\"\"\n",
    "        # Use current spacing if None provided\n",
    "        if spacing is None:\n",
    "            spacing = self.spacing\n",
    "            \n",
    "        # Get boot indexes\n",
    "        index = self.get_boot_indexes(spacing)\n",
    "        \n",
    "        # Identify dimensions\n",
    "        dims = self.data_dims\n",
    "        \n",
    "        # Extract the subsets based on bootstrap indices\n",
    "        subset = self.ds.isel({dims[0]: index[dims[0]][:, nby], dims[1]: index[dims[1]][:, nbx]})\n",
    "        \n",
    "        # Check if the required variables exist in the dataset\n",
    "        for var_name in variables_names:\n",
    "            if var_name not in subset:\n",
    "                raise ValueError(f\"Variable {var_name} not found in dataset\")\n",
    "        \n",
    "        # Get dimensions of the first variable to determine array sizes\n",
    "        ny, nx = subset[variables_names[0]].shape\n",
    "        \n",
    "        # Create results array for structure function\n",
    "        results = np.full(ny * nx, np.nan)\n",
    "        \n",
    "        # Arrays to store separation distances\n",
    "        dx_vals = np.full(ny * nx, 0.0)\n",
    "        dy_vals = np.full(ny * nx, 0.0)\n",
    "        \n",
    "        # Cache for shifted arrays\n",
    "        shift_cache = {}\n",
    "        \n",
    "        # Calculate structure function based on specified type\n",
    "        if fun == 'longitudinal':\n",
    "            results, dx_vals, dy_vals = self._calc_longitudinal(subset, variables_names, order, \n",
    "                                                              dims, ny, nx, shift_cache)\n",
    "        elif fun == 'transverse':\n",
    "            results, dx_vals, dy_vals = self._calc_transverse(subset, variables_names, order, \n",
    "                                                            dims, ny, nx, shift_cache)\n",
    "        elif fun == 'default_vel':\n",
    "            results, dx_vals, dy_vals = self._calc_default_vel(subset, variables_names, order, \n",
    "                                                             dims, ny, nx, shift_cache)\n",
    "        elif fun == 'scalar':\n",
    "            results, dx_vals, dy_vals = self._calc_scalar(subset, variables_names, order, \n",
    "                                                        dims, ny, nx, shift_cache)\n",
    "        elif fun == 'scalar_scalar':\n",
    "            results, dx_vals, dy_vals = self._calc_scalar_scalar(subset, variables_names, order, \n",
    "                                                              dims, ny, nx, shift_cache)\n",
    "        elif fun == 'longitudinal_transverse':\n",
    "            results, dx_vals, dy_vals = self._calc_longitudinal_transverse(subset, variables_names, order, \n",
    "                                                                         dims, ny, nx, shift_cache)\n",
    "        elif fun == 'longitudinal_scalar':\n",
    "            results, dx_vals, dy_vals = self._calc_longitudinal_scalar(subset, variables_names, order, \n",
    "                                                                     dims, ny, nx, shift_cache)\n",
    "        elif fun == 'transverse_scalar':\n",
    "            results, dx_vals, dy_vals = self._calc_transverse_scalar(subset, variables_names, order, \n",
    "                                                                   dims, ny, nx, shift_cache)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported function type: {fun}\")\n",
    "            \n",
    "        return results, dx_vals, dy_vals\n",
    "\n",
    "\n",
    "    \n",
    "    def monte_carlo_simulation_first_pick(self, variables_names, order, nbootstrap, fun='longitudinal', spacing=None, n_jobs=-1):\n",
    "        \"\"\"\n",
    "        Run Monte Carlo simulation for structure function calculation with multiple bootstrap samples\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        variables_names : list\n",
    "            List of variable names to use, depends on function type\n",
    "        order : int or tuple\n",
    "            Order(s) of the structure function\n",
    "        nbootstrap : int\n",
    "            Number of bootstrap samples\n",
    "        fun : str\n",
    "            Type of structure function: ['longitudinal','transverse','default_vel','scalar',\n",
    "            'scalar_scalar','longitudinal_transverse','longitudinal_scalar','transverse_scalar']\n",
    "        spacing : dict or int, optional\n",
    "            Spacing values to use. If None, uses the instance's current spacing.\n",
    "        n_jobs : int, optional\n",
    "            Number of jobs for parallel processing. Default is -1 (all cores).\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        list\n",
    "            Raw structure function values for all bootstrap samples\n",
    "        list\n",
    "            DX values for all bootstrap samples\n",
    "        list\n",
    "            DY values for all bootstrap samples\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # Use current spacing if None provided\n",
    "        if spacing is None:\n",
    "            spacing = self.spacing\n",
    "        \n",
    "        # Set the seed for reproducibility\n",
    "        np.random.seed(10000000)\n",
    "        \n",
    "        # Get boot indexes for the specified spacing\n",
    "        index = self.get_boot_indexes(spacing)\n",
    "        \n",
    "        # Generate all bootstrap indices at once\n",
    "        nbx = np.random.choice(index[self.data_dims[1]].shape[1], size=nbootstrap)\n",
    "        nby = np.random.choice(index[self.data_dims[0]].shape[1], size=nbootstrap)\n",
    "    \n",
    "        # Prepare a function to run in parallel\n",
    "        def simulate_bootstrap(j):\n",
    "            return self.calculate_structure_function(\n",
    "                variables_names=variables_names,\n",
    "                order=order,\n",
    "                fun=fun,\n",
    "                nbx=nbx[j], \n",
    "                nby=nby[j], \n",
    "                spacing=spacing\n",
    "            )\n",
    "        \n",
    "        # Run simulations in parallel (prefer threads for numpy operations)\n",
    "        results = Parallel(n_jobs=n_jobs, prefer=\"threads\", verbose=0)(\n",
    "            delayed(simulate_bootstrap)(j) for j in range(nbootstrap)\n",
    "        )\n",
    "        \n",
    "        # Unpack results\n",
    "        sf_results = [r[0] for r in results]\n",
    "        dx_vals = [r[1] for r in results]\n",
    "        dy_vals = [r[2] for r in results]\n",
    "        \n",
    "        return sf_results, dx_vals, dy_vals\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    def bin_sf(self, variables_names, order, bins, fun='longitudinal', \n",
    "               initial_nbootstrap=100, max_nbootstrap=1000, step_nbootstrap=100,\n",
    "               convergence_eps=0.1):\n",
    "        \"\"\"\n",
    "        Bin structure function results with improved weighted statistics and memory efficiency.\n",
    "        Adaptively runs bootstraps scaled by density until convergence.\n",
    "        Processes multiple spacings simultaneously and uses dx,dy as weights for statistics.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        variables_names : list\n",
    "            List of variable names to use, depends on function type\n",
    "        order : float or tuple\n",
    "            Order(s) of the structure function\n",
    "        bins : dict\n",
    "            Dictionary with dimensions as keys and bin edges as values\n",
    "        fun : str, optional\n",
    "            Type of structure function: ['longitudinal','transverse','default_vel','scalar',\n",
    "            'scalar_scalar','longitudinal_transverse','longitudinal_scalar','transverse_scalar']\n",
    "        initial_nbootstrap : int, optional\n",
    "            Initial number of bootstrap samples\n",
    "        max_nbootstrap : int, optional\n",
    "            Maximum number of bootstrap samples\n",
    "        step_nbootstrap : int, optional\n",
    "            Step size for increasing bootstrap samples\n",
    "        convergence_eps : float, optional\n",
    "            Convergence threshold for bin standard deviation\n",
    "            \n",
    "        Returns\n",
    "        --------\n",
    "        xarray.Dataset\n",
    "            Dataset with binned structure function results\n",
    "        \"\"\"\n",
    "        # Validate bins\n",
    "        if not isinstance(bins, dict):\n",
    "            raise ValueError(\"'bins' must be a dictionary with dimensions as keys and bin edges as values\")\n",
    "        \n",
    "        for dim in self.data_dims:\n",
    "            if dim not in bins:\n",
    "                raise ValueError(f\"Bins must be provided for dimension '{dim}'\")\n",
    "        \n",
    "        # Vectorized detection of bin type (log vs linear)\n",
    "        log_bins = {}\n",
    "        for dim, bin_edges in bins.items():\n",
    "            if len(bin_edges) < 2:\n",
    "                raise ValueError(f\"Bin edges for dimension '{dim}' must have at least 2 values\")\n",
    "            \n",
    "            # Calculate ratios efficiently\n",
    "            ratios = bin_edges[1:] / bin_edges[:-1]\n",
    "            ratio_std = np.std(ratios)\n",
    "            ratio_mean = np.mean(ratios)\n",
    "            \n",
    "            # Determine bin type without loops\n",
    "            if ratio_std / ratio_mean < 0.01:\n",
    "                log_bins[dim] = not (np.abs(ratio_mean - 1.0) < 0.01)\n",
    "                bin_type = \"logarithmic\" if log_bins[dim] else \"linear\"\n",
    "            else:\n",
    "                log_bins[dim] = False\n",
    "                bin_type = \"irregular (treating as linear)\"\n",
    "            print(f\"Detected {bin_type} binning for dimension '{dim}'\")\n",
    "        \n",
    "        # Get all available spacings from the class\n",
    "        spacing_values = np.array(self.get_all_spacings())\n",
    "        print(f\"Available spacings: {spacing_values}\")\n",
    "        \n",
    "        # Clear memory early\n",
    "        gc.collect()\n",
    "        \n",
    "        # Extract bin arrays\n",
    "        dims_order = self.data_dims\n",
    "        bins_x = np.array(bins[dims_order[1]])\n",
    "        bins_y = np.array(bins[dims_order[0]])\n",
    "        n_bins_x = len(bins_x) - 1\n",
    "        n_bins_y = len(bins_y) - 1\n",
    "        \n",
    "        # Calculate bin centers (vectorized)\n",
    "        if log_bins[dims_order[1]]:\n",
    "            x_centers = np.sqrt(bins_x[:-1] * bins_x[1:])  # Geometric mean\n",
    "        else:\n",
    "            x_centers = 0.5 * (bins_x[:-1] + bins_x[1:])   # Arithmetic mean\n",
    "            \n",
    "        if log_bins[dims_order[0]]:\n",
    "            y_centers = np.sqrt(bins_y[:-1] * bins_y[1:])\n",
    "        else:\n",
    "            y_centers = 0.5 * (bins_y[:-1] + bins_y[1:])\n",
    "        \n",
    "        # Pre-calculate bin areas for later use\n",
    "        bin_areas = np.zeros((n_bins_y, n_bins_x))\n",
    "        for j in range(n_bins_y):\n",
    "            for i in range(n_bins_x):\n",
    "                bin_areas[j, i] = (bins_x[i+1] - bins_x[i]) * (bins_y[j+1] - bins_y[j])\n",
    "        \n",
    "        # Initialize arrays (use numpy arrays throughout for better performance)\n",
    "        sf_totals = np.zeros((n_bins_y, n_bins_x))\n",
    "        weight_totals = np.zeros((n_bins_y, n_bins_x))\n",
    "        sf_sq_totals = np.zeros((n_bins_y, n_bins_x))\n",
    "        bootstrap_counts = np.zeros((n_bins_y, n_bins_x), dtype=int)\n",
    "        point_counts = np.zeros((n_bins_y, n_bins_x), dtype=int)\n",
    "        \n",
    "        # Initialize bin tracking\n",
    "        bin_status = np.zeros((n_bins_y, n_bins_x), dtype=bool)  # True = converged\n",
    "        bin_density = np.zeros((n_bins_y, n_bins_x))\n",
    "        bin_bootstraps = np.ones((n_bins_y, n_bins_x), dtype=int) * initial_nbootstrap\n",
    "        \n",
    "        # Use dictionaries of numpy arrays for spacing tracking\n",
    "        spacing_bootstraps = {sp: initial_nbootstrap // len(spacing_values) for sp in spacing_values}\n",
    "        spacing_point_counts = {sp: np.zeros((n_bins_y, n_bins_x), dtype=int) for sp in spacing_values}\n",
    "        spacing_effectiveness = {sp: np.zeros((n_bins_y, n_bins_x)) for sp in spacing_values}\n",
    "        bin_spacing_bootstraps = {sp: np.ones((n_bins_y, n_bins_x), dtype=int) * \n",
    "                               (initial_nbootstrap // len(spacing_values)) for sp in spacing_values}\n",
    "    \n",
    "        # Optimized function to process spacing data\n",
    "        def process_spacing_data(sp_value, bootstraps, add_to_counts=True):\n",
    "            \"\"\"Optimized data processing function\"\"\"\n",
    "            if bootstraps <= 0:\n",
    "                return np.zeros((n_bins_y, n_bins_x), dtype=int)\n",
    "                \n",
    "            # Create spacing dictionary once\n",
    "            spacing = {dim: sp_value for dim in self.data_dims}\n",
    "            \n",
    "            # Run Monte Carlo simulation \n",
    "            sf_results, dx_vals, dy_vals = self.monte_carlo_simulation_first_pick(\n",
    "                variables_names=variables_names,\n",
    "                order=order, \n",
    "                nbootstrap=bootstraps, \n",
    "                fun=fun, \n",
    "                spacing=spacing\n",
    "            )\n",
    "            \n",
    "            # Initialize counters\n",
    "            batch_counts = np.zeros((n_bins_y, n_bins_x), dtype=int)\n",
    "            bin_points_added = np.zeros((n_bins_y, n_bins_x), dtype=int)\n",
    "            \n",
    "            # Process bootstrap samples (optimize for large arrays)\n",
    "            for b in range(len(sf_results)):\n",
    "                sf = sf_results[b]\n",
    "                dx = dx_vals[b]\n",
    "                dy = dy_vals[b]\n",
    "                \n",
    "                # Create mask for valid values (vectorized)\n",
    "                valid = ~np.isnan(dx) & ~np.isnan(dy) & ~np.isnan(sf)\n",
    "                if not np.any(valid):\n",
    "                    continue\n",
    "                    \n",
    "                # Extract valid data (reduces array sizes for subsequent operations)\n",
    "                sf_valid = sf[valid]\n",
    "                dx_valid = dx[valid]\n",
    "                dy_valid = dy[valid]\n",
    "                \n",
    "                # Calculate weights\n",
    "                weights = np.sqrt(dx_valid**2 + dy_valid**2)\n",
    "                \n",
    "                # Find bin indices (vectorized)\n",
    "                x_idx = np.digitize(dx_valid, bins_x) - 1\n",
    "                y_idx = np.digitize(dy_valid, bins_y) - 1\n",
    "                \n",
    "                # Filter for valid bin indices (vectorized)\n",
    "                valid_bins = (x_idx >= 0) & (x_idx < n_bins_x) & (y_idx >= 0) & (y_idx < n_bins_y)\n",
    "                if not np.any(valid_bins):\n",
    "                    continue\n",
    "                \n",
    "                # Extract only data that falls within valid bins\n",
    "                valid_sf = sf_valid[valid_bins]\n",
    "                valid_weights = weights[valid_bins]\n",
    "                valid_x_idx = x_idx[valid_bins]\n",
    "                valid_y_idx = y_idx[valid_bins]\n",
    "                \n",
    "                # Use numpy unique with return_counts for faster binning\n",
    "                bin_ids = valid_y_idx * n_bins_x + valid_x_idx\n",
    "                unique_bins, bin_indices, bin_counts = np.unique(bin_ids, return_inverse=True, return_counts=True)\n",
    "                \n",
    "                # Process each unique bin (using numpy's advanced indexing)\n",
    "                for idx, bin_id in enumerate(unique_bins):\n",
    "                    j, i = bin_id // n_bins_x, bin_id % n_bins_x\n",
    "                    bin_mask = bin_indices == idx\n",
    "                    \n",
    "                    # Get values for this bin\n",
    "                    bin_sf = valid_sf[bin_mask]\n",
    "                    bin_weights = valid_weights[bin_mask]\n",
    "                    \n",
    "                    # Count points for density calculation\n",
    "                    if add_to_counts:\n",
    "                        added_points = bin_counts[idx]\n",
    "                        point_counts[j, i] += added_points\n",
    "                        bin_points_added[j, i] += added_points\n",
    "                        spacing_point_counts[sp_value][j, i] += added_points\n",
    "                        batch_counts[j, i] += 1\n",
    "                    \n",
    "                    # Efficient weight normalization\n",
    "                    weight_sum = np.sum(bin_weights)\n",
    "                    if weight_sum > 0:\n",
    "                        norm_weights = bin_weights / weight_sum\n",
    "                        \n",
    "                        # Update weighted accumulators\n",
    "                        sf_totals[j, i] += np.sum(bin_sf * norm_weights)\n",
    "                        sf_sq_totals[j, i] += np.sum((bin_sf**2) * norm_weights)\n",
    "                        weight_totals[j, i] += 1\n",
    "            \n",
    "            # Update spacing effectiveness (vectorized where possible)\n",
    "            if add_to_counts and bootstraps > 0:\n",
    "                mask = bin_points_added > 0\n",
    "                if np.any(mask):\n",
    "                    spacing_effectiveness[sp_value][mask] = bin_points_added[mask] / bootstraps\n",
    "            \n",
    "            # Clear memory\n",
    "            del sf_results, dx_vals, dy_vals\n",
    "            gc.collect()\n",
    "            \n",
    "            return batch_counts\n",
    "    \n",
    "        # Optimized statistics calculation\n",
    "        def calculate_bin_statistics():\n",
    "            \"\"\"Vectorized statistics calculation\"\"\"\n",
    "            means = np.full((n_bins_y, n_bins_x), np.nan)\n",
    "            stds = np.full((n_bins_y, n_bins_x), np.nan)\n",
    "            \n",
    "            # Apply vectorized operations where possible\n",
    "            valid_bins = weight_totals > 0\n",
    "            means[valid_bins] = sf_totals[valid_bins] / weight_totals[valid_bins]\n",
    "            \n",
    "            valid_var_bins = weight_totals > 1\n",
    "            if np.any(valid_var_bins):\n",
    "                # Vectorized variance calculation\n",
    "                variance = np.zeros_like(sf_totals)\n",
    "                variance[valid_var_bins] = (sf_sq_totals[valid_var_bins] / weight_totals[valid_var_bins]) - (means[valid_var_bins]**2)\n",
    "                \n",
    "                # Ensure non-negative variance\n",
    "                stds[valid_var_bins] = np.sqrt(np.maximum(0, variance[valid_var_bins]))\n",
    "            \n",
    "            return means, stds\n",
    "        \n",
    "        # Efficient convergence check\n",
    "        def update_bin_convergence(sf_stds):\n",
    "            \"\"\"Update convergence status for all bins\"\"\"\n",
    "            # Create mask of unconverged bins\n",
    "            mask = ~bin_status & (point_counts > 0) & (bin_bootstraps < max_nbootstrap)\n",
    "            \n",
    "            # Track which bins we need to update\n",
    "            updated_bins = []\n",
    "            \n",
    "            # Find converged bins based on standard deviation\n",
    "            std_converged = mask & (sf_stds < convergence_eps)\n",
    "            if np.any(std_converged):\n",
    "                for j, i in zip(*np.where(std_converged)):\n",
    "                    print(f\"    Bin ({j},{i}) converged with std {sf_stds[j,i]:.4f}\")\n",
    "                    updated_bins.append((j, i))\n",
    "                bin_status[std_converged] = True\n",
    "                mask[std_converged] = False\n",
    "            \n",
    "            # Find bins that reached max bootstraps\n",
    "            max_reached = mask & (bin_bootstraps >= max_nbootstrap)\n",
    "            if np.any(max_reached):\n",
    "                for j, i in zip(*np.where(max_reached)):\n",
    "                    print(f\"    Bin ({j},{i}) reached max bootstraps, using best estimate with std {sf_stds[j,i]:.4f}\")\n",
    "                    updated_bins.append((j, i))\n",
    "                bin_status[max_reached] = True\n",
    "                mask[max_reached] = False\n",
    "            \n",
    "            return mask, updated_bins\n",
    "        \n",
    "        # Process initial bootstraps for all spacings (optimized)\n",
    "        print(\"Processing initial bootstraps for all spacings\")\n",
    "        init_samples_per_spacing = max(1, initial_nbootstrap // len(spacing_values))\n",
    "        \n",
    "        for sp_value in spacing_values:\n",
    "            print(f\"  Processing spacing {sp_value} with {init_samples_per_spacing} samples\")\n",
    "            batch_counts = process_spacing_data(sp_value, init_samples_per_spacing, add_to_counts=True)\n",
    "            bootstrap_counts += batch_counts\n",
    "            spacing_bootstraps[sp_value] = init_samples_per_spacing\n",
    "        \n",
    "        # Calculate initial density (vectorized where possible)\n",
    "        total_points = np.sum(point_counts)\n",
    "        if total_points > 0:\n",
    "            # Use broadcasting for faster calculation\n",
    "            bin_density = np.zeros((n_bins_y, n_bins_x))\n",
    "            mask = (bin_areas > 0) & (point_counts > 0)\n",
    "            bin_density[mask] = point_counts[mask] / (bin_areas[mask] * total_points)\n",
    "        \n",
    "        # Normalize densities efficiently\n",
    "        max_density = np.max(bin_density) if np.max(bin_density) > 0 else 1.0\n",
    "        bin_density /= max_density\n",
    "        \n",
    "        # Calculate bootstrap steps (vectorized)\n",
    "        bootstrap_steps = np.maximum(\n",
    "            step_nbootstrap, \n",
    "            (step_nbootstrap * (1 + 2 * bin_density)).astype(int)\n",
    "        )\n",
    "        \n",
    "        # Calculate initial statistics\n",
    "        sf_means, sf_stds = calculate_bin_statistics()\n",
    "        \n",
    "        # Initial convergence check\n",
    "        unconverged_bins, _ = update_bin_convergence(sf_stds)\n",
    "        total_unconverged = np.sum(unconverged_bins)\n",
    "        print(f\"Starting with {total_unconverged} bins to converge\")\n",
    "        \n",
    "        # Main adaptive bootstrapping loop (optimized)\n",
    "        iteration = 1\n",
    "        \n",
    "        while np.any(unconverged_bins):\n",
    "            print(f\"\\nIteration {iteration} - {np.sum(unconverged_bins)} unconverged bins remaining\")\n",
    "            \n",
    "            # Get unconverged bin indices and densities (vectorized)\n",
    "            j_indices, i_indices = np.where(unconverged_bins)\n",
    "            densities = bin_density[j_indices, i_indices]\n",
    "            \n",
    "            # Sort bins by density (highest first)\n",
    "            sort_idx = np.argsort(densities)[::-1]\n",
    "            j_indices = j_indices[sort_idx]\n",
    "            i_indices = i_indices[sort_idx]\n",
    "            \n",
    "            # Process each bin\n",
    "            for idx in range(len(j_indices)):\n",
    "                j, i = j_indices[idx], i_indices[idx]\n",
    "                density = bin_density[j, i]\n",
    "                \n",
    "                # Skip if already converged\n",
    "                if bin_status[j, i]:\n",
    "                    continue\n",
    "                    \n",
    "                # Get bootstrap step for this bin\n",
    "                step = bootstrap_steps[j, i]\n",
    "                print(f\"  Processing bin ({j},{i}) with density {density:.3f}\")\n",
    "                \n",
    "                # Calculate spacing weights based on effectiveness (vectorized where possible)\n",
    "                effectiveness_values = np.array([spacing_effectiveness[sp][j, i] for sp in spacing_values])\n",
    "                \n",
    "                # Normalize weights\n",
    "                total_effectiveness = np.sum(effectiveness_values)\n",
    "                if total_effectiveness > 0:\n",
    "                    spacing_weights = effectiveness_values / total_effectiveness\n",
    "                else:\n",
    "                    spacing_weights = np.ones(len(spacing_values)) / len(spacing_values)\n",
    "                    \n",
    "                # Calculate remaining bootstraps\n",
    "                remaining_bootstraps = max_nbootstrap - bin_bootstraps[j, i]\n",
    "                \n",
    "                # Calculate additional bootstraps per spacing (vectorized)\n",
    "                additional_bootstraps = np.minimum(\n",
    "                    (step * spacing_weights).astype(int),\n",
    "                    remaining_bootstraps\n",
    "                )\n",
    "                \n",
    "                # Process additional bootstraps for each spacing\n",
    "                total_additional = 0\n",
    "                for sp_idx, sp_value in enumerate(spacing_values):\n",
    "                    sp_additional = additional_bootstraps[sp_idx]\n",
    "                    \n",
    "                    if sp_additional > 0:\n",
    "                        print(f\"    Adding {sp_additional} bootstraps for spacing {sp_value}\")\n",
    "                        \n",
    "                        # Process additional bootstraps\n",
    "                        batch_counts = process_spacing_data(sp_value, sp_additional, add_to_counts=False)\n",
    "                        bootstrap_counts[j, i] += np.sum(batch_counts[j, i])\n",
    "                        \n",
    "                        # Update tracking variables\n",
    "                        spacing_bootstraps[sp_value] += sp_additional\n",
    "                        bin_spacing_bootstraps[sp_value][j, i] += sp_additional\n",
    "                        total_additional += sp_additional\n",
    "                \n",
    "                # Update bootstrap count\n",
    "                bin_bootstraps[j, i] += total_additional\n",
    "                \n",
    "                # Check if max bootstraps reached\n",
    "                if bin_bootstraps[j, i] >= max_nbootstrap:\n",
    "                    bin_status[j, i] = True\n",
    "                    print(f\"    Bin ({j},{i}) reached max bootstraps, using best estimate\")\n",
    "                    unconverged_bins[j, i] = False\n",
    "            \n",
    "            # Recalculate statistics for all bins\n",
    "            sf_means, sf_stds = calculate_bin_statistics()\n",
    "            \n",
    "            # Update convergence status\n",
    "            unconverged_bins, _ = update_bin_convergence(sf_stds)\n",
    "            \n",
    "            # Clean memory\n",
    "            gc.collect()\n",
    "            iteration += 1\n",
    "        \n",
    "        # Create xarray Dataset (final step - no changes needed)\n",
    "        coord_dims = {\n",
    "            dims_order[1]: x_centers,\n",
    "            dims_order[0]: y_centers\n",
    "        }\n",
    "        \n",
    "        ds_binned = xr.Dataset(\n",
    "            data_vars={\n",
    "                'sf': ((dims_order[0], dims_order[1]), sf_means),\n",
    "                'sf_std': ((dims_order[0], dims_order[1]), sf_stds),\n",
    "                'nbootstraps': ((dims_order[0], dims_order[1]), bin_bootstraps),\n",
    "                'density': ((dims_order[0], dims_order[1]), bin_density),\n",
    "                'point_counts': ((dims_order[0], dims_order[1]), point_counts),\n",
    "                'converged': ((dims_order[0], dims_order[1]), bin_status)\n",
    "            },\n",
    "            coords=coord_dims,\n",
    "            attrs={\n",
    "                'bin_type_x': 'logarithmic' if log_bins[dims_order[1]] else 'linear',\n",
    "                'bin_type_y': 'logarithmic' if log_bins[dims_order[0]] else 'linear',\n",
    "                'convergence_eps': convergence_eps,\n",
    "                'max_nbootstrap': max_nbootstrap,\n",
    "                'initial_nbootstrap': initial_nbootstrap,\n",
    "                'order': str(order),\n",
    "                'function_type': fun,\n",
    "                'spacing_values': list(spacing_values),\n",
    "                'variables': variables_names\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Add bin edges to the dataset\n",
    "        ds_binned[f'{dims_order[1]}_bins'] = ((dims_order[1], 'edge'), np.column_stack([bins_x[:-1], bins_x[1:]]))\n",
    "        ds_binned[f'{dims_order[0]}_bins'] = ((dims_order[0], 'edge'), np.column_stack([bins_y[:-1], bins_y[1:]]))\n",
    "        \n",
    "        # Add spacing effectiveness information\n",
    "        for sp_value in spacing_values:\n",
    "            ds_binned[f'eff_spacing_{sp_value}'] = ((dims_order[0], dims_order[1]), spacing_effectiveness[sp_value])\n",
    "            ds_binned[f'bs_spacing_{sp_value}'] = ((dims_order[0], dims_order[1]), bin_spacing_bootstraps[sp_value])\n",
    "        \n",
    "        return ds_binned\n",
    "\n",
    "    def get_isotropic_sf(self, variables_names, order=2.0, bins=None,\n",
    "                       initial_nbootstrap=100, max_nbootstrap=1000, \n",
    "                       step_nbootstrap=100, fun='longitudinal', \n",
    "                       n_bins_theta=36, window_size_theta=None, window_size_r=None,\n",
    "                       convergence_eps=0.1, n_jobs=-1):\n",
    "        \"\"\"\n",
    "        Get isotropic (radially binned) structure function results.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        variables_names : list\n",
    "            List of variable names to use\n",
    "        order : float or tuple\n",
    "            Order(s) of the structure function\n",
    "        bins : dict\n",
    "            Dictionary with 'r' as key and bin edges as values\n",
    "        initial_nbootstrap : int\n",
    "            Initial number of bootstrap samples\n",
    "        max_nbootstrap : int\n",
    "            Maximum number of bootstrap samples\n",
    "        step_nbootstrap : int\n",
    "            Step size for increasing bootstrap samples\n",
    "        fun : str\n",
    "            Type of structure function\n",
    "        n_bins_theta : int\n",
    "            Number of angular bins\n",
    "        window_size_theta : int\n",
    "            Window size for theta bootstrapping\n",
    "        window_size_r : int\n",
    "            Window size for radial bootstrapping\n",
    "        convergence_eps : float\n",
    "            Convergence threshold\n",
    "        n_jobs : int\n",
    "            Number of jobs for parallel processing\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        xarray.Dataset\n",
    "            Dataset with isotropic structure function results\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # Validate bins\n",
    "        if bins is None or 'r' not in bins:\n",
    "            raise ValueError(\"'bins' must be a dictionary with 'r' as key and bin edges as values\")\n",
    "        \n",
    "        r_bins = np.array(bins['r'])\n",
    "        if len(r_bins) < 2:\n",
    "            raise ValueError(\"Bin edges for 'r' must have at least 2 values\")\n",
    "        \n",
    "        # Determine bin type (log or linear)\n",
    "        ratios = r_bins[1:] / r_bins[:-1]\n",
    "        ratio_std = np.std(ratios)\n",
    "        ratio_mean = np.mean(ratios)\n",
    "        \n",
    "        if ratio_std / ratio_mean < 0.01:\n",
    "            if np.abs(ratio_mean - 1.0) < 0.01:\n",
    "                log_bins = False  # Linear bins\n",
    "                r_centers = 0.5 * (r_bins[:-1] + r_bins[1:])\n",
    "                print(\"Detected linear binning for radial dimension\")\n",
    "            else:\n",
    "                log_bins = True  # Log bins\n",
    "                r_centers = np.sqrt(r_bins[:-1] * r_bins[1:])\n",
    "                print(\"Detected logarithmic binning for radial dimension\")\n",
    "        else:\n",
    "            log_bins = False  # Default to linear if irregular spacing\n",
    "            r_centers = 0.5 * (r_bins[:-1] + r_bins[1:])\n",
    "            print(\"Detected irregular bin spacing for radial dimension, treating as linear\")\n",
    "        \n",
    "        n_bins_r = len(r_centers)\n",
    "        \n",
    "        # Default window sizes if not provided\n",
    "        if window_size_theta is None:\n",
    "            window_size_theta = max(n_bins_theta // 3, 1)\n",
    "        if window_size_r is None:\n",
    "            window_size_r = max(n_bins_r // 3, 1)\n",
    "        \n",
    "        print(f\"Using {n_bins_r} radial bins and {n_bins_theta} angular bins\")\n",
    "        print(f\"Using window size {window_size_theta} for theta and {window_size_r} for r\")\n",
    "        \n",
    "        # Set up angular bins (full circle)\n",
    "        theta_bins = np.linspace(-np.pi, np.pi, n_bins_theta + 1)\n",
    "        theta_centers = 0.5 * (theta_bins[:-1] + theta_bins[1:])\n",
    "        \n",
    "        # Get available spacings\n",
    "        all_spacings = self.get_all_spacings()\n",
    "        print(f\"Available spacings: {all_spacings}\")\n",
    "        \n",
    "        # Initialize accumulators for weighted statistics - EXACTLY like bin_sf\n",
    "        sf_totals = np.zeros(n_bins_r)          # Sum(sf * weight)\n",
    "        weight_totals = np.zeros(n_bins_r)      # Sum(weight)\n",
    "        sf_sq_totals = np.zeros(n_bins_r)       # Sum(sf^2 * weight)\n",
    "        bin_point_counts = np.zeros(n_bins_r, dtype=int)  # Points per bin\n",
    "        bootstrap_counts = np.zeros(n_bins_r, dtype=int)  # Bootstraps per bin\n",
    "        \n",
    "        # Arrays for angular-radial bins\n",
    "        sfr = np.full((n_bins_theta, n_bins_r), np.nan)  # Angular-radial values\n",
    "        sfr_counts = np.zeros((n_bins_theta, n_bins_r))  # Counts per bin\n",
    "        \n",
    "        # Track effectiveness of each spacing for each bin\n",
    "        spacing_effectiveness = {sp: np.zeros(n_bins_r) for sp in all_spacings}\n",
    "        spacing_bootstraps = {sp: np.zeros(n_bins_r, dtype=int) for sp in all_spacings}\n",
    "        \n",
    "        # Function to process Monte Carlo results\n",
    "        def process_bootstrap_results(sf_results, dx_vals, dy_vals, spacing_value):\n",
    "            for b in range(len(sf_results)):\n",
    "                sf = sf_results[b]\n",
    "                dx = dx_vals[b]\n",
    "                dy = dy_vals[b]\n",
    "                \n",
    "                # Skip invalid values\n",
    "                valid = ~np.isnan(sf) & ~np.isnan(dx) & ~np.isnan(dy)\n",
    "                sf_valid = sf[valid]\n",
    "                dx_valid = dx[valid]\n",
    "                dy_valid = dy[valid]\n",
    "                \n",
    "                if len(sf_valid) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Convert to polar coordinates\n",
    "                r_valid = np.sqrt(dx_valid**2 + dy_valid**2)\n",
    "                theta_valid = np.arctan2(dy_valid, dx_valid)\n",
    "                \n",
    "                # Use r as weight (separation distance)\n",
    "                weights = r_valid\n",
    "                \n",
    "                # Bin by radius\n",
    "                for j in range(n_bins_r):\n",
    "                    r_min, r_max = r_bins[j], r_bins[j+1]\n",
    "                    r_mask = (r_valid >= r_min) & (r_valid < r_max)\n",
    "                    \n",
    "                    if np.any(r_mask):\n",
    "                        # Calculate weighted statistics for this radial bin\n",
    "                        bin_sf = sf_valid[r_mask]\n",
    "                        bin_weights = weights[r_mask]\n",
    "                        weight_sum = np.sum(bin_weights)\n",
    "                        \n",
    "                        if weight_sum > 0:\n",
    "                            # Normalize weights\n",
    "                            norm_weights = bin_weights / weight_sum\n",
    "                            \n",
    "                            # Update accumulators - EXACTLY like bin_sf\n",
    "                            weighted_sf = np.sum(bin_sf * norm_weights)\n",
    "                            weighted_sf_sq = np.sum((bin_sf**2) * norm_weights)\n",
    "                            \n",
    "                            sf_totals[j] += weighted_sf\n",
    "                            sf_sq_totals[j] += weighted_sf_sq\n",
    "                            weight_totals[j] += 1  # Count this bootstrap\n",
    "                            bin_point_counts[j] += len(bin_sf)\n",
    "                            bootstrap_counts[j] += 1\n",
    "                            \n",
    "                            # Track effectiveness for this spacing\n",
    "                            spacing_effectiveness[spacing_value][j] += len(bin_sf)\n",
    "                            spacing_bootstraps[spacing_value][j] += 1\n",
    "                            \n",
    "                        # Bin by angle within this radius\n",
    "                        for i in range(n_bins_theta):\n",
    "                            theta_min, theta_max = theta_bins[i], theta_bins[i+1]\n",
    "                            theta_mask = r_mask & (theta_valid >= theta_min) & (theta_valid < theta_max)\n",
    "                            \n",
    "                            if np.any(theta_mask):\n",
    "                                theta_sf = sf_valid[theta_mask]\n",
    "                                theta_weights = weights[theta_mask]\n",
    "                                theta_weight_sum = np.sum(theta_weights)\n",
    "                                \n",
    "                                if theta_weight_sum > 0:\n",
    "                                    # Normalize weights\n",
    "                                    theta_norm_weights = theta_weights / theta_weight_sum\n",
    "                                    theta_weighted_sf = np.sum(theta_sf * theta_norm_weights)\n",
    "                                    \n",
    "                                    # Update angular-radial bin with weighted average\n",
    "                                    if np.isnan(sfr[i, j]):\n",
    "                                        sfr[i, j] = theta_weighted_sf\n",
    "                                    else:\n",
    "                                        # Weighted average with previous value\n",
    "                                        sfr[i, j] = (sfr[i, j] * sfr_counts[i, j] + theta_weighted_sf) / (sfr_counts[i, j] + 1)\n",
    "                                    \n",
    "                                    sfr_counts[i, j] += 1\n",
    "        \n",
    "        # Distribute initial bootstraps across spacings - like bin_sf\n",
    "        for sp_value in all_spacings:\n",
    "            spacing = {dim: sp_value for dim in self.data_dims}\n",
    "            \n",
    "            # Calculate bootstraps for this spacing\n",
    "            init_bootstraps = max(5, initial_nbootstrap // len(all_spacings))\n",
    "            print(f\"Running {init_bootstraps} initial bootstraps for spacing {sp_value}\")\n",
    "            \n",
    "            # Use monte_carlo_simulation_first_pick - EXACTLY like bin_sf\n",
    "            sf_results, dx_vals, dy_vals = self.monte_carlo_simulation_first_pick(\n",
    "                variables_names=variables_names,\n",
    "                order=order,\n",
    "                nbootstrap=init_bootstraps,\n",
    "                fun=fun,\n",
    "                spacing=spacing,\n",
    "                n_jobs=n_jobs\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            process_bootstrap_results(sf_results, dx_vals, dy_vals, sp_value)\n",
    "            \n",
    "            # Clean up memory\n",
    "            del sf_results, dx_vals, dy_vals\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calculate bin densities for adaptive step sizes\n",
    "        bin_areas = np.pi * (r_bins[1:]**2 - r_bins[:-1]**2)\n",
    "        bin_densities = np.zeros(n_bins_r)\n",
    "        \n",
    "        for j in range(n_bins_r):\n",
    "            if bin_areas[j] > 0:\n",
    "                bin_densities[j] = bin_point_counts[j] / bin_areas[j]\n",
    "        \n",
    "        # Normalize densities to [0,1]\n",
    "        max_density = np.max(bin_densities) if np.max(bin_densities) > 0 else 1.0\n",
    "        bin_densities = bin_densities / max_density\n",
    "        \n",
    "        # Mark bins with too few points as converged\n",
    "        bin_converged = np.zeros(n_bins_r, dtype=bool)\n",
    "        sfa = np.full(n_bins_r, np.nan)  # Final SF values\n",
    "        err = np.full(n_bins_r, np.nan)  # Standard deviation\n",
    "        \n",
    "        # Initialize convergence tracking\n",
    "        for j in range(n_bins_r):\n",
    "            if bin_point_counts[j] < 10:\n",
    "                bin_converged[j] = True\n",
    "                print(f\"Bin {j} (r={r_centers[j]:.2f}) has only {bin_point_counts[j]} points - marked as converged\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate initial statistics\n",
    "            if weight_totals[j] > 0:\n",
    "                weighted_mean = sf_totals[j] / weight_totals[j]\n",
    "                \n",
    "                if weight_totals[j] > 1:\n",
    "                    weighted_var = (sf_sq_totals[j] / weight_totals[j]) - (weighted_mean**2)\n",
    "                    weighted_std = np.sqrt(max(0, weighted_var))\n",
    "                else:\n",
    "                    weighted_std = float('inf')\n",
    "                \n",
    "                # Store initial estimates\n",
    "                sfa[j] = weighted_mean\n",
    "                err[j] = weighted_std\n",
    "                \n",
    "                # Check for early convergence\n",
    "                if weighted_std <= convergence_eps:\n",
    "                    bin_converged[j] = True\n",
    "                    print(f\"Bin {j} (r={r_centers[j]:.2f}) converged early with std {weighted_std:.4f}\")\n",
    "        \n",
    "        # Main adaptive convergence loop - continues until all bins converge or reach max_nbootstrap\n",
    "        iteration = 1\n",
    "        \n",
    "        while True:\n",
    "            # Check for termination condition\n",
    "            unconverged_bins = np.where(~bin_converged & (bootstrap_counts < max_nbootstrap))[0]\n",
    "            if len(unconverged_bins) == 0:\n",
    "                print(\"All bins have either converged or reached max_nbootstrap\")\n",
    "                break\n",
    "                \n",
    "            print(f\"\\nIteration {iteration} - {len(unconverged_bins)} unconverged bins remaining\")\n",
    "            \n",
    "            # Sort unconverged bins by density (highest first) - EXACTLY like bin_sf\n",
    "            unconverged_bins = sorted(unconverged_bins, key=lambda j: bin_densities[j], reverse=True)\n",
    "            \n",
    "            # Group bootstraps by spacing for efficiency\n",
    "            spacing_to_run = {}\n",
    "            \n",
    "            for j in unconverged_bins:\n",
    "                # Calculate adaptive bootstrap step based on density - EXACTLY like bin_sf\n",
    "                bootstrap_step = max(\n",
    "                    step_nbootstrap, \n",
    "                    int(step_nbootstrap * (1 + 2 * bin_densities[j]))\n",
    "                )\n",
    "                \n",
    "                # Cap to max_nbootstrap\n",
    "                bootstrap_step = min(bootstrap_step, max_nbootstrap - bootstrap_counts[j])\n",
    "                \n",
    "                if bootstrap_step <= 0:\n",
    "                    continue\n",
    "                    \n",
    "                print(f\"  Bin {j} (r={r_centers[j]:.2f}) - Adding {bootstrap_step} bootstraps\")\n",
    "                \n",
    "                # Calculate effectiveness for each spacing for this bin\n",
    "                eff_dict = {}\n",
    "                total_eff = 0\n",
    "                \n",
    "                for sp in all_spacings:\n",
    "                    # Calculate points per bootstrap\n",
    "                    if spacing_bootstraps[sp][j] > 0:\n",
    "                        eff = spacing_effectiveness[sp][j] / spacing_bootstraps[sp][j]\n",
    "                    else:\n",
    "                        # If no data for this spacing, give it a small baseline chance\n",
    "                        eff = 0.\n",
    "                    \n",
    "                    eff_dict[sp] = eff\n",
    "                    total_eff += eff\n",
    "                \n",
    "                # Choose spacing based on effectiveness or ensure at least 1 bootstrap\n",
    "                # when effectiveness is zero\n",
    "                if total_eff > 0:\n",
    "                    # Distribute bootstraps proportionally to effectiveness\n",
    "                    for sp, eff in eff_dict.items():\n",
    "                        # Calculate bootstraps for this spacing based on effectiveness\n",
    "                        sp_bootstraps = max(0, int(bootstrap_step * (eff / total_eff)))\n",
    "                        \n",
    "                        if sp not in spacing_to_run:\n",
    "                            spacing_to_run[sp] = 0\n",
    "                        \n",
    "                        spacing_to_run[sp] += sp_bootstraps\n",
    "                else:\n",
    "                    # If all spacings have zero effectiveness, distribute evenly\n",
    "                    per_spacing = max(1, bootstrap_step // len(all_spacings))\n",
    "                    for sp in all_spacings:\n",
    "                        if sp not in spacing_to_run:\n",
    "                            spacing_to_run[sp] = 0\n",
    "                        \n",
    "                        spacing_to_run[sp] += per_spacing\n",
    "            \n",
    "            # Run additional bootstraps for each spacing\n",
    "            for sp_value, nbootstrap in spacing_to_run.items():\n",
    "                if nbootstrap <= 0:\n",
    "                    continue\n",
    "                    \n",
    "                print(f\"  Running {nbootstrap} additional bootstraps for spacing {sp_value}\")\n",
    "                \n",
    "                # Use monte_carlo_simulation_first_pick - EXACTLY like bin_sf\n",
    "                spacing = {dim: sp_value for dim in self.data_dims}\n",
    "                \n",
    "                sf_results, dx_vals, dy_vals = self.monte_carlo_simulation_first_pick(\n",
    "                    variables_names=variables_names,\n",
    "                    order=order,\n",
    "                    nbootstrap=nbootstrap,\n",
    "                    fun=fun,\n",
    "                    spacing=spacing,\n",
    "                    n_jobs=n_jobs\n",
    "                )\n",
    "                \n",
    "                # Process results - adds only the additional bootstraps\n",
    "                process_bootstrap_results(sf_results, dx_vals, dy_vals, sp_value)\n",
    "                \n",
    "                # Clean up memory\n",
    "                del sf_results, dx_vals, dy_vals\n",
    "                gc.collect()\n",
    "            \n",
    "            # Update statistics and check for convergence\n",
    "            for j in unconverged_bins:\n",
    "                if weight_totals[j] > 0:\n",
    "                    weighted_mean = sf_totals[j] / weight_totals[j]\n",
    "                    \n",
    "                    if weight_totals[j] > 1:\n",
    "                        weighted_var = (sf_sq_totals[j] / weight_totals[j]) - (weighted_mean**2)\n",
    "                        weighted_std = np.sqrt(max(0, weighted_var))\n",
    "                    else:\n",
    "                        weighted_std = float('inf')\n",
    "                    \n",
    "                    # Update current estimates\n",
    "                    sfa[j] = weighted_mean\n",
    "                    err[j] = weighted_std\n",
    "                    \n",
    "                    # Check convergence\n",
    "                    if weighted_std <= convergence_eps:\n",
    "                        bin_converged[j] = True\n",
    "                        print(f\"  Bin {j} (r={r_centers[j]:.2f}) converged with std {weighted_std:.4f}\")\n",
    "                    elif bootstrap_counts[j] >= max_nbootstrap:\n",
    "                        bin_converged[j] = True\n",
    "                        print(f\"  Bin {j} (r={r_centers[j]:.2f}) reached max bootstraps, using best estimate\")\n",
    "            \n",
    "            iteration += 1\n",
    "        \n",
    "        # Ensure final values for all bins\n",
    "        for j in range(n_bins_r):\n",
    "            if weight_totals[j] > 0:\n",
    "                weighted_mean = sf_totals[j] / weight_totals[j]\n",
    "                \n",
    "                if weight_totals[j] > 1:\n",
    "                    weighted_var = (sf_sq_totals[j] / weight_totals[j]) - (weighted_mean**2)\n",
    "                    weighted_std = np.sqrt(max(0, weighted_var))\n",
    "                else:\n",
    "                    weighted_std = np.nan\n",
    "                \n",
    "                sfa[j] = weighted_mean\n",
    "                err[j] = weighted_std\n",
    "        \n",
    "        # Calculate error metrics\n",
    "        # Error of isotropy\n",
    "        eiso = np.zeros(n_bins_r)\n",
    "        \n",
    "        # Create sliding windows for theta bootstrapping\n",
    "        indices_theta = sliding_window_view(\n",
    "            np.arange(n_bins_theta), \n",
    "            (n_bins_theta - window_size_theta + 1,), \n",
    "            writeable=False\n",
    "        )[::1]\n",
    "        \n",
    "        n_samples_theta = len(indices_theta)\n",
    "        \n",
    "        for i in range(n_samples_theta):\n",
    "            idx = indices_theta[i]\n",
    "            mean_sf = bn.nanmean(sfr[idx, :], axis=0)\n",
    "            eiso += np.abs(mean_sf - sfa)\n",
    "        \n",
    "        eiso /= n_samples_theta\n",
    "        \n",
    "        # Create sliding windows for r bootstrapping\n",
    "        indices_r = sliding_window_view(\n",
    "            np.arange(n_bins_r), \n",
    "            (n_bins_r - window_size_r + 1,), \n",
    "            writeable=False\n",
    "        )[::1]\n",
    "        \n",
    "        n_samples_r = len(indices_r)\n",
    "        \n",
    "        # Use a subset of bins for homogeneity\n",
    "        r_subset = r_centers[indices_r[0]]\n",
    "        \n",
    "        # Calculate mean across all angles\n",
    "        meanh = np.zeros(len(r_subset))\n",
    "        ehom = np.zeros(len(r_subset))\n",
    "        \n",
    "        for i in range(n_samples_r):\n",
    "            idx = indices_r[i]\n",
    "            meanh += bn.nanmean(sfr[:, idx], axis=0)\n",
    "        \n",
    "        meanh /= n_samples_r\n",
    "        \n",
    "        for i in range(n_samples_r):\n",
    "            idx = indices_r[i]\n",
    "            ehom += np.abs(bn.nanmean(sfr[:, idx], axis=0) - meanh)\n",
    "        \n",
    "        ehom /= n_samples_r\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        confidence_level = 0.95\n",
    "        z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "        ci_upper = sfa + z_score * err / np.sqrt(n_bins_theta)\n",
    "        ci_lower = sfa - z_score * err / np.sqrt(n_bins_theta)\n",
    "        \n",
    "        # Create output dataset\n",
    "        ds_iso = xr.Dataset(\n",
    "            data_vars={\n",
    "                'sf_polar': (('theta', 'r'), sfr),  # Angular-radial values\n",
    "                'sf': (('r'), sfa),                 # Isotropic values\n",
    "                'error_isotropy': (('r'), eiso),    # Isotropy error\n",
    "                'std': (('r'), err),                # Standard deviation\n",
    "                'ci_upper': (('r'), ci_upper),      # Upper confidence interval\n",
    "                'ci_lower': (('r'), ci_lower),      # Lower confidence interval\n",
    "                'error_homogeneity': (('r_subset'), ehom),  # Homogeneity error\n",
    "                'mean_homogeneity': (('r_subset'), meanh),  # Mean homogeneity\n",
    "                'n_bootstrap': (('r'), bootstrap_counts),   # Bootstrap counts\n",
    "                'bin_density': (('r'), bin_densities),      # Bin densities\n",
    "                'bin_samples': (('r'), bin_point_counts)    # Point counts\n",
    "            },\n",
    "            coords={\n",
    "                'r': r_centers,\n",
    "                'r_subset': r_subset,\n",
    "                'theta': theta_centers,\n",
    "                'r_bins': r_bins,\n",
    "                'theta_bins': theta_bins\n",
    "            },\n",
    "            attrs={\n",
    "                'order': order,\n",
    "                'function_type': fun,\n",
    "                'window_size_theta': window_size_theta,\n",
    "                'window_size_r': window_size_r,\n",
    "                'convergence_eps': convergence_eps,\n",
    "                'max_nbootstrap': max_nbootstrap,\n",
    "                'initial_nbootstrap': initial_nbootstrap,\n",
    "                'bin_type': 'logarithmic' if log_bins else 'linear',\n",
    "                'variables': variables_names\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Add spacing effectiveness information\n",
    "        for sp in all_spacings:\n",
    "            # Calculate effectiveness (points per bootstrap)\n",
    "            eff = np.zeros(n_bins_r)\n",
    "            for j in range(n_bins_r):\n",
    "                if spacing_bootstraps[sp][j] > 0:\n",
    "                    eff[j] = spacing_effectiveness[sp][j] / spacing_bootstraps[sp][j]\n",
    "            \n",
    "            ds_iso[f'effectiveness_spacing_{sp}'] = (('r'), eff)\n",
    "            ds_iso[f'bootstraps_spacing_{sp}'] = (('r'), spacing_bootstraps[sp])\n",
    "        \n",
    "        return ds_iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b17457-b8d2-4e3e-b287-22361136132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_isotropic_sf(self, analysis_results=None, order=2.0, \n",
    "                       initial_nbootstrap=100, max_nbootstrap=1000, \n",
    "                       step_nbootstrap=100, fun='lon', \n",
    "                       n_bins_r=16, n_bins_theta=36, \n",
    "                       window_size_theta=None, window_size_r=None,\n",
    "                       convergence_eps=0.1):\n",
    "        \"\"\"\n",
    "        Get isotropic (radially binned) structure function results and calculate \n",
    "        errors of isotropy and homogeneity through bootstrapping.\n",
    "    \n",
    "        This function directly bins the results in radius r = sqrt(dx**2 + dy**2)\n",
    "        using adaptive bootstrapping based on bin density until convergence.\n",
    "    \n",
    "        Parameters\n",
    "        -----------\n",
    "        analysis_results : dict, optional\n",
    "            Results from analyze_all_spacings. If None, will run the analysis.\n",
    "        order : int, optional\n",
    "            Order of the structure function\n",
    "        initial_nbootstrap : int, optional\n",
    "            Initial number of bootstrap samples\n",
    "        max_nbootstrap : int, optional\n",
    "            Maximum number of bootstrap samples\n",
    "        step_nbootstrap : int, optional\n",
    "            Step size for increasing bootstrap samples\n",
    "        fun : str, optional\n",
    "            Type of structure function ('lon', 'trans', 'cu', etc.). Default is 'lon'.\n",
    "        n_bins_r : int, optional\n",
    "            Number of radial bins\n",
    "        n_bins_theta : int, optional\n",
    "            Number of angular bins\n",
    "        window_size_theta : int, optional\n",
    "            Window size for theta bootstrapping. Defaults to n_bins_theta//3.\n",
    "        window_size_r : int, optional\n",
    "            Window size for radial bootstrapping. Defaults to n_bins_r//3.\n",
    "        convergence_eps : float, optional\n",
    "            Convergence threshold for bin standard deviation\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        xarray.Dataset\n",
    "            Dataset with isotropic structure function results and error metrics\n",
    "        \"\"\"\n",
    "    \n",
    "        # Default window sizes if not provided\n",
    "        if window_size_theta is None:\n",
    "            window_size_theta = max(n_bins_theta // 3, 1)\n",
    "        if window_size_r is None:\n",
    "            window_size_r = max(n_bins_r // 3, 1)\n",
    "        \n",
    "        print(f\"Using {n_bins_r} radial bins and {n_bins_theta} angular bins\")\n",
    "        print(f\"Using window size {window_size_theta} for theta and {window_size_r} for r\")\n",
    "        print(f\"Using convergence threshold of {convergence_eps}\")\n",
    "    \n",
    "        # First, get all available spacings if not already calculated\n",
    "        all_spacings = self.get_all_spacings()\n",
    "        print(f\"Automatically determined spacings: {all_spacings}\")\n",
    "    \n",
    "        # Run initial analysis if not provided\n",
    "        \n",
    "        print(f\"Running initial analysis for {fun} with {initial_nbootstrap} bootstrap samples...\")\n",
    "        # Run analysis for the function type\n",
    "        analysis_results = self.analyze_all_spacings(order, initial_nbootstrap, fun)\n",
    "    \n",
    "        # Collect all dx, dy values to create bins\n",
    "        dx_values = []\n",
    "        dy_values = []\n",
    "    \n",
    "        for sp_value in all_spacings:\n",
    "            if sp_value in analysis_results:\n",
    "                result = analysis_results[sp_value]\n",
    "                dx_values.extend(result['DX'].flatten())\n",
    "                dy_values.extend(result['DY'].flatten())\n",
    "        \n",
    "        dx_values = np.array(dx_values)\n",
    "        dy_values = np.array(dy_values)\n",
    "    \n",
    "        # Calculate radial and angular values\n",
    "        r_values = np.sqrt(dx_values**2 + dy_values**2)\n",
    "        theta_values = np.arctan2(dy_values, dx_values)\n",
    "    \n",
    "        # Create logarithmic radial bins\n",
    "        r_min = np.min(r_values[r_values > 0])\n",
    "        r_max = np.max(r_values)\n",
    "        r_bins = np.logspace(np.log10(r_min), np.log10(r_max), n_bins_r + 1)\n",
    "        r_centers = np.sqrt(r_bins[:-1] * r_bins[1:])  # Geometric mean\n",
    "    \n",
    "        # Create angular bins\n",
    "        theta_bins = np.linspace(-np.pi, np.pi, n_bins_theta + 1)\n",
    "        theta_centers = 0.5 * (theta_bins[:-1] + theta_bins[1:])\n",
    "    \n",
    "        # Create a combined array of all sf values and their corresponding r, theta\n",
    "        all_dx = []\n",
    "        all_dy = []\n",
    "        all_sf = []\n",
    "    \n",
    "        for sp_value in all_spacings:\n",
    "            if sp_value in analysis_results:\n",
    "                result = analysis_results[sp_value]\n",
    "                dx = result['DX'].flatten()\n",
    "                dy = result['DY'].flatten()\n",
    "                sf = result['sf_list'].flatten()\n",
    "            \n",
    "                # Skip NaN values\n",
    "                valid = ~np.isnan(sf)\n",
    "                all_dx.extend(dx[valid])\n",
    "                all_dy.extend(dy[valid])\n",
    "                all_sf.extend(sf[valid])\n",
    "    \n",
    "        all_dx = np.array(all_dx)\n",
    "        all_dy = np.array(all_dy)\n",
    "        all_sf = np.array(all_sf)\n",
    "    \n",
    "        if len(all_sf) == 0:\n",
    "            print(f\"Warning: No valid data for {fun}\")\n",
    "            # Return empty dataset\n",
    "            ds_iso = xr.Dataset(\n",
    "                data_vars={},\n",
    "                coords={\n",
    "                    'r': r_centers,\n",
    "                    'theta': theta_centers,\n",
    "                },\n",
    "                attrs={\n",
    "                    'order': order,\n",
    "                    'function_type': fun,\n",
    "                    'error': 'No valid data found'\n",
    "                }\n",
    "            )\n",
    "            return ds_iso\n",
    "        \n",
    "        # Convert to polar coordinates\n",
    "        all_r = np.sqrt(all_dx**2 + all_dy**2)\n",
    "        all_theta = np.arctan2(all_dy, all_dx)\n",
    "        \n",
    "        # Calculate bin point counts and densities\n",
    "        bin_point_counts = np.zeros(n_bins_r)\n",
    "        \n",
    "        for j, (r_min, r_max) in enumerate(zip(r_bins[:-1], r_bins[1:])):\n",
    "            # Mask for this radial bin (regardless of angle)\n",
    "            radial_mask = (all_r >= r_min) & (all_r < r_max)\n",
    "            bin_point_counts[j] = np.sum(radial_mask)\n",
    "            print(f\"Bin {j} (r={r_centers[j]:.2f}) has {bin_point_counts[j]} total samples\")\n",
    "        \n",
    "        # Calculate bin areas and densities\n",
    "        bin_areas = np.pi * (r_bins[1:]**2 - r_bins[:-1]**2)  # Area of annular sections\n",
    "        bin_densities = bin_point_counts / bin_areas\n",
    "        \n",
    "        # Normalize densities to [0,1] for easier bootstrap scaling\n",
    "        if np.max(bin_densities) > 0:\n",
    "            normalized_densities = bin_densities / np.max(bin_densities)\n",
    "        else:\n",
    "            normalized_densities = np.zeros_like(bin_densities)\n",
    "        \n",
    "        # Initialize bin data structure for tracking convergence\n",
    "        confidence_level = 0.95 # % 95 percent confidence interval\n",
    "        \n",
    "        z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "        \n",
    "        bin_data = {}\n",
    "        \n",
    "        for j in range(n_bins_r):\n",
    "            # Scale initial bootstrap count based on density\n",
    "            # Higher density bins start with more bootstraps\n",
    "            \n",
    "            density_factor = 1.0 + normalized_densities[j]  # Range: 1.0-2.0\n",
    "            scaled_initial_bootstrap = max(\n",
    "                initial_nbootstrap,\n",
    "                int(initial_nbootstrap * density_factor)\n",
    "            )\n",
    "            scaled_initial_bootstrap = min(scaled_initial_bootstrap, max_nbootstrap)\n",
    "            \n",
    "            bin_data[j] = {\n",
    "                'sf_values': [],  # Structure function values for this radial bin across all angles\n",
    "                'converged': False,  # Convergence flag\n",
    "                'current_nbootstrap': scaled_initial_bootstrap,  # Current bootstrap count for this bin\n",
    "                'density': normalized_densities[j],  # Normalized density\n",
    "                'bootstrap_factor': density_factor  # Factor for scaling bootstrap steps\n",
    "            }\n",
    "            \n",
    "            # Immediately mark bins with too few samples as converged\n",
    "            if bin_point_counts[j] < 10:\n",
    "                bin_data[j]['converged'] = True\n",
    "                print(f\"Bin {j} (r={r_centers[j]:.2f}) has only {bin_point_counts[j]} samples - marked as converged\")\n",
    "            else:\n",
    "                print(f\"Bin {j} (r={r_centers[j]:.2f}) - Density: {normalized_densities[j]:.3f}, Initial bootstraps: {scaled_initial_bootstrap}\")\n",
    "        \n",
    "        # Process bins until all converge or reach max bootstraps\n",
    "        unconverged_bins = set(j for j in range(n_bins_r) if not bin_data[j]['converged'])\n",
    "        \n",
    "        # Initialize result arrays\n",
    "        sfr = np.full((n_bins_theta, n_bins_r), np.nan)  # Angular-radial values\n",
    "        sfa = np.full(n_bins_r, np.nan)  # Isotropic (angular-averaged) values\n",
    "        err = np.full(n_bins_r, np.nan)  # Standard deviation\n",
    "        n_bootstrap_per_bin = np.zeros(n_bins_r, dtype=int)  # Number of bootstraps used\n",
    "        \n",
    "        iteration = 1\n",
    "        while unconverged_bins and any(bin_data[bin_idx]['current_nbootstrap'] <= max_nbootstrap for bin_idx in unconverged_bins):\n",
    "            print(f\"\\nIteration {iteration} - {len(unconverged_bins)} unconverged bins remaining\")\n",
    "            \n",
    "            # Sort unconverged bins by density (highest density first)\n",
    "            sorted_bins = sorted(unconverged_bins, \n",
    "                                key=lambda bin_idx: bin_data[bin_idx]['density'],\n",
    "                                reverse=True)\n",
    "            \n",
    "            # Process bins in order of decreasing density\n",
    "            for j in sorted_bins:\n",
    "                current_nbootstrap = bin_data[j]['current_nbootstrap']\n",
    "                \n",
    "                if current_nbootstrap > max_nbootstrap:\n",
    "                    print(f\"  Bin {j} (r={r_centers[j]:.2f}) exceeded max bootstraps\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"  Processing bin {j} (r={r_centers[j]:.2f}) with nbootstrap = {current_nbootstrap} (density: {bin_data[j]['density']:.3f})\")\n",
    "                \n",
    "                # Clear previous data for this bin\n",
    "                bin_data[j]['sf_values'] = []\n",
    "                \n",
    "                # Check if we need to run additional bootstraps\n",
    "                if current_nbootstrap > initial_nbootstrap:\n",
    "                    # Run analysis with current bootstrap count for this bin\n",
    "                    print(f\"    Running analysis with {current_nbootstrap} bootstrap samples...\")\n",
    "                    analysis_results = self.analyze_all_spacings(order, current_nbootstrap, fun)\n",
    "                \n",
    "                # Reset structure function array for this radial bin\n",
    "                sfr_bin = np.full(n_bins_theta, np.nan)\n",
    "                \n",
    "                # Process angular bins\n",
    "                for i, (theta_min, theta_max) in enumerate(zip(theta_bins[:-1], theta_bins[1:])):\n",
    "                    # Mask for this angular-radial bin\n",
    "                    r_min, r_max = r_bins[j], r_bins[j+1]\n",
    "                    mask = (all_r >= r_min) & (all_r < r_max) & (all_theta >= theta_min) & (all_theta < theta_max)\n",
    "                    \n",
    "                    if np.sum(mask) > 0:\n",
    "                        # Store average value for this angular-radial bin\n",
    "                        sf_values = all_sf[mask]\n",
    "                        sfr_bin[i] = np.mean(sf_values)\n",
    "                        \n",
    "                        # Add data to bin collection for convergence checking\n",
    "                        bin_data[j]['sf_values'].extend(sf_values)\n",
    "                \n",
    "                # Update the full array\n",
    "                sfr[:, j] = sfr_bin\n",
    "                \n",
    "                # Calculate isotropic (azimuthally averaged) value for this radial bin\n",
    "                if len(bin_data[j]['sf_values']) > 0:\n",
    "                    sf_array = np.array(bin_data[j]['sf_values'])\n",
    "                    bin_mean = bn.nanmean(sf_array)\n",
    "                    bin_std = bn.nanstd(sf_array)\n",
    "                    \n",
    "                    # Calculate relative standard deviation for convergence check\n",
    "                    rel_std = bin_std \n",
    "                    \n",
    "                    # Check if error is within threshold\n",
    "                    if rel_std <= convergence_eps:\n",
    "                        print(f\"    Bin {j} (r={r_centers[j]:.2f}) converged with {current_nbootstrap} bootstraps\")\n",
    "                        print(f\"    std: {rel_std:.4f}, threshold: {convergence_eps}\")\n",
    "                        bin_data[j]['converged'] = True\n",
    "                        sfa[j] = bin_mean\n",
    "                        err[j] = bin_std\n",
    "                        n_bootstrap_per_bin[j] = current_nbootstrap\n",
    "                        unconverged_bins.remove(j)\n",
    "                    else:\n",
    "                        # Increase bootstrap count based on density\n",
    "                        # Higher density bins get larger bootstrap steps\n",
    "                        density_factor = bin_data[j]['bootstrap_factor']\n",
    "                        bootstrap_step = int(step_nbootstrap * density_factor)\n",
    "                        \n",
    "                        bin_data[j]['current_nbootstrap'] += bootstrap_step\n",
    "                        bin_data[j]['current_nbootstrap'] = min(bin_data[j]['current_nbootstrap'], max_nbootstrap)\n",
    "                        print(f\"    Bin {j} (r={r_centers[j]:.2f}) NOT converged, increased to {bin_data[j]['current_nbootstrap']} bootstraps (step: {bootstrap_step})\")\n",
    "                        print(f\"    std: {rel_std:.4f}, threshold: {convergence_eps}\")\n",
    "                else:\n",
    "                    # No data in this bin\n",
    "                    print(f\"    Bin {j} (r={r_centers[j]:.2f}) has no data points\")\n",
    "                    sfa[j] = np.nan\n",
    "                    err[j] = np.nan\n",
    "                    n_bootstrap_per_bin[j] = 0\n",
    "                    unconverged_bins.remove(j)  # Remove bins with no data\n",
    "            \n",
    "            iteration += 1\n",
    "        \n",
    "        # For bins that didn't converge, use the best available estimates\n",
    "        for j in unconverged_bins:\n",
    "            if len(bin_data[j]['sf_values']) > 0:\n",
    "                print(f\"  Bin {j} (r={r_centers[j]:.2f}) did not converge, using best estimate with {bin_data[j]['current_nbootstrap']} bootstraps\")\n",
    "                sf_array = np.array(bin_data[j]['sf_values'])\n",
    "                sfa[j] = bn.nanmean(sf_array)\n",
    "                err[j] = bn.nanstd(sf_array)\n",
    "                n_bootstrap_per_bin[j] = bin_data[j]['current_nbootstrap']\n",
    "        \n",
    "        # Calculate error metrics based on the final sfr array\n",
    "        # Error of isotropy - bootstrapping in theta\n",
    "        eiso = np.zeros(n_bins_r)\n",
    "    \n",
    "        # Create sliding windows for theta bootstrapping\n",
    "        indices_theta = sliding_window_view(\n",
    "            np.arange(n_bins_theta), \n",
    "            (n_bins_theta - window_size_theta + 1,), \n",
    "            writeable=False\n",
    "        )[::1]\n",
    "    \n",
    "        n_samples_theta = len(indices_theta)\n",
    "    \n",
    "        for i in range(n_samples_theta):\n",
    "            idx = indices_theta[i]\n",
    "            # Calculate mean across this window of theta\n",
    "            mean_sf = bn.nanmean(sfr[idx, :], axis=0)\n",
    "        \n",
    "            # Accumulate absolute deviation from isotropic mean\n",
    "            eiso += np.abs(mean_sf - sfa)\n",
    "        \n",
    "        # Normalize\n",
    "        eiso /= n_samples_theta\n",
    "    \n",
    "        # Error of homogeneity - bootstrapping in r\n",
    "        # Create sliding windows for r bootstrapping\n",
    "        indices_r = sliding_window_view(\n",
    "            np.arange(n_bins_r), \n",
    "            (n_bins_r - window_size_r + 1,), \n",
    "            writeable=False\n",
    "        )[::1]\n",
    "    \n",
    "        n_samples_r = len(indices_r)\n",
    "    \n",
    "        # Use a subset of r bins for homogeneity calculation\n",
    "        r_subset = r_centers[indices_r[0]]\n",
    "    \n",
    "        # Calculate mean across all angles for each r window\n",
    "        meanh = np.zeros(len(r_subset))\n",
    "        ehom = np.zeros(len(r_subset))\n",
    "    \n",
    "        for i in range(n_samples_r):\n",
    "            idx = indices_r[i]\n",
    "            meanh += bn.nanmean(sfr[:, idx], axis=0)\n",
    "        \n",
    "        meanh /= n_samples_r\n",
    "    \n",
    "        for i in range(n_samples_r):\n",
    "            idx = indices_r[i]\n",
    "            ehom += np.abs(bn.nanmean(sfr[:, idx], axis=0) - meanh)\n",
    "        \n",
    "        ehom /= n_samples_r\n",
    "    \n",
    "        # Calculate confidence intervals\n",
    "        ci_upper = sfa + 1.96 * err / np.sqrt(n_bins_theta)\n",
    "        ci_lower = sfa - 1.96 * err / np.sqrt(n_bins_theta)\n",
    "    \n",
    "        # Create xarray Dataset\n",
    "        ds_iso = xr.Dataset(\n",
    "            data_vars={\n",
    "                # Full polar grid\n",
    "                'sf_polar': (('theta', 'r'), sfr),\n",
    "            \n",
    "                # Isotropic results\n",
    "                'sf': (('r'), sfa),\n",
    "            \n",
    "                # Error metrics\n",
    "                'error_isotropy': (('r'), eiso),\n",
    "                'std': (('r'), err),\n",
    "                'ci_upper': (('r'), ci_upper),\n",
    "                'ci_lower': (('r'), ci_lower),\n",
    "            \n",
    "                # Homogeneity error\n",
    "                'error_homogeneity': (('r_subset'), ehom),\n",
    "                'mean_homogeneity': (('r_subset'), meanh),\n",
    "                \n",
    "                # Convergence information\n",
    "                'n_bootstrap': (('r'), n_bootstrap_per_bin),\n",
    "                'bin_density': (('r'), normalized_densities),\n",
    "                'bin_samples': (('r'), bin_point_counts)\n",
    "            },\n",
    "            coords={\n",
    "                'r': r_centers,\n",
    "                'r_subset': r_subset,\n",
    "                'theta': theta_centers,\n",
    "                'r_bins': r_bins,\n",
    "                'theta_bins': theta_bins\n",
    "            },\n",
    "            attrs={\n",
    "                'order': order,\n",
    "                'function_type': fun,\n",
    "                'window_size_theta': window_size_theta,\n",
    "                'window_size_r': window_size_r,\n",
    "                'n_bins_r': n_bins_r,\n",
    "                'n_bins_theta': n_bins_theta,\n",
    "                'convergence_eps': convergence_eps,\n",
    "                'max_nbootstrap': max_nbootstrap,\n",
    "                'initial_nbootstrap': initial_nbootstrap\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        return ds_iso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3868a-619b-4f3d-b719-600841f8b28e",
   "metadata": {},
   "source": [
    "# Compute 2D Longitudinal SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfdb600-8454-4295-85f2-5b1b918768e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dx_min = 150.0\n",
    "dy_min = 150.0\n",
    "dx_max = 150.0e3\n",
    "dy_max = 150.0e3\n",
    "nbins = 16\n",
    "# Initialize with automatic spacing detection\n",
    "sfun = SFun(ds, bootsize={'x': 32, 'y': 32})\n",
    "\n",
    "# Get binned results with automatic adaptive bootstrapping\n",
    "# This will automatically determine spacings, run analysis for all spacings,\n",
    "# and adaptively increase bootstraps until convergence\n",
    "\n",
    "results_ds = sfun.bin_sf(\n",
    "    variables_names=['u','v'],\n",
    "    order = 2.0,\n",
    "    bins = {'x':np.logspace(np.log10(dx_min),np.log10(dx_max),nbins+1),'y':np.logspace(np.log10(dy_min),np.log10(dy_max),nbins+1)},\n",
    "    fun = 'longitudinal',\n",
    "    initial_nbootstrap=10,\n",
    "    max_nbootstrap=100,\n",
    "    step_nbootstrap=50,\n",
    "    convergence_eps=0.1\n",
    ")\n",
    "\n",
    "# Access results as an xarray Dataset\n",
    "print(results_ds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6f6ad-8c96-4687-8515-bfd605e7fd44",
   "metadata": {},
   "source": [
    "# Plots 2D Longitudinal SF, errors and number of bootstraps/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba57415-18d8-4357-ae57-307e2b83b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the structure function\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pcolormesh(results_ds.x, results_ds.y, results_ds.sf.T, cmap='viridis', shading='gouraud')\n",
    "plt.colorbar(label='Structure Function')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('rx')\n",
    "plt.ylabel('ry')\n",
    "plt.title('Structure Function Analysis')\n",
    "\n",
    "# Plot the structure function\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pcolormesh(results_ds.x, results_ds.y, results_ds.sf_std.T, cmap='viridis', shading='auto')\n",
    "plt.colorbar(label='Structure Function')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('rx')\n",
    "plt.ylabel('ry')\n",
    "plt.title('Structure Function Standard Error')\n",
    "\n",
    "\n",
    "# Plot the number of bootstraps used for convergence\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pcolormesh(results_ds.x, results_ds.y, results_ds.nbootstraps.T, cmap='plasma', shading='auto')\n",
    "plt.colorbar(label='Number of Bootstraps')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('rx')\n",
    "plt.ylabel('ry')\n",
    "plt.title('Bootstraps Required for Convergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec06cf7-207e-47fa-812e-0e1c1d6a3302",
   "metadata": {},
   "source": [
    "# Compute Isotropic SF (lon & trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c57a0-32b8-4492-a118-a1b63f64601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nbins = 20\n",
    "r_b = np.logspace(np.log10(np.sqrt(dx_min**2 + dy_min**2)),np.log10(np.sqrt((dx_max-dx_min)**2 + (dy_max-dy_min)**2)),nbins+1)\n",
    "sfun = SFun(ds, bootsize={'x': 32, 'y': 32})\n",
    "\n",
    "# First, calculate the longitudinal isotropic structure function\n",
    "iso_lon = sfun.get_isotropic_sf(\n",
    "    ['u','v'], \n",
    "    order = 2.0, \n",
    "    bins={'r':r_b},\n",
    "    initial_nbootstrap=10,\n",
    "    max_nbootstrap=100,\n",
    "    step_nbootstrap=50,\n",
    "    fun='longitudinal',           # Calculate longitudinal structure function\n",
    "    n_bins_theta=36,     # Use 36 angular bins (10° each)\n",
    "    window_size_theta=12,# Window size for theta bootstrapping\n",
    "    window_size_r=8,     # Window size for r bootstrapping\n",
    "    convergence_eps=0.1\n",
    ")\n",
    "\n",
    "# First, calculate the longitudinal isotropic structure function\n",
    "iso_trans = sfun.get_isotropic_sf(\n",
    "    ['u','v'], \n",
    "    order = 2.0, \n",
    "    bins={'r':r_b},\n",
    "    initial_nbootstrap=10,\n",
    "    max_nbootstrap=100,\n",
    "    step_nbootstrap=50,\n",
    "    fun='transverse',           # Calculate transverse structure function\n",
    "    n_bins_theta=36,     # Use 36 angular bins (10° each)\n",
    "    window_size_theta=12,# Window size for theta bootstrapping\n",
    "    window_size_r=8,     # Window size for r bootstrapping\n",
    "    convergence_eps=0.1\n",
    ")\n",
    "\n",
    "# Print basic information about the results\n",
    "print(\"Longitudinal Structure Function:\")\n",
    "print(iso_lon)\n",
    "\n",
    "print(\"\\nTransverse Structure Function:\")\n",
    "print(iso_trans)\n",
    "\n",
    "# Access specific components\n",
    "print(\"\\nRadial grid points:\", iso_lon.r.values)\n",
    "print(\"Angular grid points:\", iso_lon.theta.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4106ab",
   "metadata": {},
   "source": [
    "# Plot Transverse, Longitudinal SFs, isotropy and homogeneity errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d952ebf-8de5-49a9-b3e0-6baf6603c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Plot 1: Isotropic Structure Functions\n",
    "# --------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot longitudinal structure function with confidence interval\n",
    "plt.loglog(iso_lon.r, iso_lon.sf, 'b-', linewidth=2, label='Longitudinal')\n",
    "plt.fill_between(iso_lon.r, \n",
    "                iso_lon.ci_lower, \n",
    "                iso_lon.ci_upper, \n",
    "                alpha=0.3, color='blue')\n",
    "\n",
    "# Plot transverse structure function with confidence interval\n",
    "plt.loglog(iso_trans.r, iso_trans.sf, 'r-', linewidth=2, label='Transverse')\n",
    "plt.fill_between(iso_trans.r, \n",
    "                iso_trans.ci_lower, \n",
    "                iso_trans.ci_upper, \n",
    "                alpha=0.3, color='red')\n",
    "\n",
    "# Add reference lines for Kolmogorov scaling (r^2/3)\n",
    "r_range = iso_lon.r.values\n",
    "k_scaling = r_range**(2/3) * iso_lon.sf.values[0] / r_range[0]**(2/3)\n",
    "plt.loglog(r_range, k_scaling, 'k--', linewidth=1.5, label=r'$r^{2/3}$ (Kolmogorov)')\n",
    "\n",
    "plt.xlabel('r', fontsize=12)\n",
    "plt.ylabel('Structure Function', fontsize=12)\n",
    "plt.title('Isotropic Structure Functions', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.6)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Plot 2: Errors of Isotropy and Homogeneity\n",
    "# --------------------------------------------------\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Error of isotropy\n",
    "plt.subplot(121)\n",
    "plt.plot(iso_lon.r, iso_lon.error_isotropy, 'b-', linewidth=2, label='Longitudinal')\n",
    "plt.plot(iso_trans.r, iso_trans.error_isotropy, 'r-', linewidth=2, label='Transverse')\n",
    "plt.xlabel('r', fontsize=12)\n",
    "plt.ylabel('Error of Isotropy', fontsize=12)\n",
    "plt.title('Error of Isotropy\\n(Variation with Angle)', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Error of homogeneity\n",
    "plt.subplot(122)\n",
    "plt.plot(iso_lon.r_subset, iso_lon.error_homogeneity, 'b-', linewidth=2, label='Longitudinal')\n",
    "plt.plot(iso_trans.r_subset, iso_trans.error_homogeneity, 'r-', linewidth=2, label='Transverse')\n",
    "plt.xlabel('r', fontsize=12)\n",
    "plt.ylabel('Error of Homogeneity', fontsize=12)\n",
    "plt.title('Error of Homogeneity\\n(Variation with Radius)', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Plot 3: Polar representation of structure functions\n",
    "# --------------------------------------------------\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Longitudinal structure function\n",
    "plt.subplot(121, polar=True)\n",
    "pcm = plt.pcolormesh(iso_lon.theta, iso_lon.r, iso_lon.sf_polar.T, \n",
    "                    cmap='viridis', shading='gouraud')\n",
    "plt.title('Longitudinal SF (Polar)')\n",
    "cbar = plt.colorbar(pcm, pad=0.15)\n",
    "cbar.set_label('Structure Function Value')\n",
    "\n",
    "# Transverse structure function\n",
    "plt.subplot(122, polar=True)\n",
    "pcm = plt.pcolormesh(iso_trans.theta, iso_trans.r, iso_trans.sf_polar.T, \n",
    "                    cmap='viridis', shading='gouraud')\n",
    "plt.title('Transverse SF (Polar)')\n",
    "cbar = plt.colorbar(pcm, pad=0.15)\n",
    "cbar.set_label('Structure Function Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Plot 4: Ratio of longitudinal to transverse\n",
    "# --------------------------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate ratio (excluding potential division by zero)\n",
    "ratio = iso_lon.sf / iso_trans.sf\n",
    "isotropic_ratio = 2.0  # For isotropic turbulence, longitudinal:transverse = 2:1\n",
    "\n",
    "plt.semilogx(iso_lon.r, ratio, 'g-', linewidth=2, label='Ratio')\n",
    "plt.axhline(y=isotropic_ratio, color='k', linestyle='--', label='Isotropic (2.0)')\n",
    "\n",
    "plt.xlabel('r', fontsize=12)\n",
    "plt.ylabel('Longitudinal / Transverse Ratio', fontsize=12)\n",
    "plt.title('Ratio of Longitudinal to Transverse Structure Functions', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643479ff-9375-46e9-bc94-703cb63435cc",
   "metadata": {},
   "source": [
    "# Compute Isotropic SF $\\delta u \\delta T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5a487-538a-4a12-ae03-465a52cdae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nbins = 20\n",
    "r_b = np.logspace(np.log10(np.sqrt(dx_min**2 + dy_min**2)),np.log10(np.sqrt((dx_max-dx_min)**2 + (dy_max-dy_min)**2)),nbins+1)\n",
    "sfun = SFun(ds, bootsize={'x': 32, 'y': 32})\n",
    "\n",
    "# First, calculate the longitudinal isotropic structure function\n",
    "iso_lon = sfun.get_isotropic_sf(\n",
    "    ['u','temperature'], \n",
    "    order = (1.0,1.0), \n",
    "    bins={'r':r_b},\n",
    "    initial_nbootstrap=10,\n",
    "    max_nbootstrap=100,\n",
    "    step_nbootstrap=50,\n",
    "    fun='scalar_scalar',           # Calculate longitudinal structure function\n",
    "    n_bins_theta=36,     # Use 36 angular bins (10° each)\n",
    "    window_size_theta=12,# Window size for theta bootstrapping\n",
    "    window_size_r=8,     # Window size for r bootstrapping\n",
    "    convergence_eps=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e46970e-0b13-47a2-9e38-9e035b27a885",
   "metadata": {},
   "source": [
    "# Plot Isotropic SF $\\delta u \\delta T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477a301-1915-4e96-8c3c-337759828b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Plot 1: Isotropic Structure Functions\n",
    "# --------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot longitudinal structure function with confidence interval\n",
    "plt.loglog(iso_lon.r, iso_lon.sf, 'b-', linewidth=2, label='Longitudinal')\n",
    "plt.fill_between(iso_lon.r, \n",
    "                iso_lon.ci_lower, \n",
    "                iso_lon.ci_upper, \n",
    "                alpha=0.3, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99a1c33-067d-4f6e-b693-1565fe644eb5",
   "metadata": {},
   "source": [
    "# Compute Isotropic SF $\\delta u \\delta(ADV_u)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b8de8-b255-4c00-8689-b7e27a3c7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nbins = 20\n",
    "r_b = np.logspace(np.log10(np.sqrt(dx_min**2 + dy_min**2)),np.log10(np.sqrt((dx_max-dx_min)**2 + (dy_max-dy_min)**2)),nbins+1)\n",
    "sfun = SFun(ds, bootsize={'x': 32, 'y': 32})\n",
    "\n",
    "# First, calculate the longitudinal isotropic structure function\n",
    "iso_lon = sfun.get_isotropic_sf(\n",
    "    ['u','adv_u'], \n",
    "    order = (1.0,1.0), \n",
    "    bins={'r':r_b},\n",
    "    initial_nbootstrap=10,\n",
    "    max_nbootstrap=100,\n",
    "    step_nbootstrap=50,\n",
    "    fun='scalar_scalar',           # Calculate longitudinal structure function\n",
    "    n_bins_theta=36,     # Use 36 angular bins (10° each)\n",
    "    window_size_theta=12,# Window size for theta bootstrapping\n",
    "    window_size_r=8,     # Window size for r bootstrapping\n",
    "    convergence_eps=1.0e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd865f4-3c1e-45aa-88bd-4f1ad2d21765",
   "metadata": {},
   "source": [
    "# Plot Isotropic SF $\\delta u \\delta(ADV_u)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d05f6e-7de5-4148-a16c-458c440b4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Plot 1: Isotropic Structure Functions\n",
    "# --------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot longitudinal structure function with confidence interval\n",
    "plt.plot(iso_lon.r, iso_lon.sf, 'b-', linewidth=2, label='Longitudinal')\n",
    "plt.fill_between(iso_lon.r, \n",
    "                iso_lon.ci_lower, \n",
    "                iso_lon.ci_upper, \n",
    "                alpha=0.3, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc45d7-2859-4ce0-a8f1-4290f25021d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
